# 利用urllib进行简单的网页抓取

 <br>
 <br>
 <br>
 

## 1. 网络爬虫的定义

网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。举一个简单的例子，我们在浏览器的地址栏中输入的字符串就是URL，例如：https://www.baidu.com/

URL就是同意资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)：

protocol :// hostname[:port] / path / [;parameters][?query]#fragment

URL的格式由三部分组成：

(1)protocol：第一部分就是协议，例如百度使用的就是https协议；

(2)hostname[:port]：第二部分就是主机名(还有端口号为可选参数)，一般网站默认的端口号为80，例如百度的主机名就是www.baidu.com，这个就是服务器的地址;

(3)path：第三部分就是主机资源的具体地址，如目录和文件名等。

网络爬虫就是根据这个URL来获取网页信息的。

## 2. 简单爬虫实例


在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：

1.urllib.request模块是用来打开和读取URLs的；

2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；

3.urllib.parse模块包含了一些解析URLs的方法；

4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。

### 2.1 urlopen()打开网站

我们使用urllib.request.urlopen()这个接口函数就可以很轻松的打开一个网站，读取并打印信息。

urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。

了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：

```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    response = request.urlopen("http://fanyi.baidu.com")
    html = response.read()
    print(html)

```

urllib使用使用request.urlopen()打开和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()，将读到的信息打印出来。

运行程序ctrl+b，可以在Sublime中查看运行结果，如下：

![](https://img-blog.csdn.net/20170228211015577?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

也可以在cmd(控制台)中输入指令：

```
python urllib_test01.py

```

运行py文件，输出信息是一样的，如下：

![](https://img-blog.csdn.net/20170228211048828?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其实这就是浏览器接收到的信息，只不过我们在使用浏览器的时候，浏览器已经将这些信息转化成了界面信息供我们浏览。当然这些代码我们也可以从浏览器中查看到。例如，使用谷歌浏览器，在任意界面单击右键选择检查，也就是审查元素(不是所有页面都可以审查元素的，例如起点中文网付费章节就不行.)，以百度界面为例，截图如下：

![](https://img-blog.csdn.net/20170228211116438?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


### 2.2 网页的信息进行解码

#### 2.2.1 通过简单的decode()命令将网页的信息进行解码

回归正题，虽然我们已经成功获取了信息，但是显然他们都是二进制的乱码，看起来很不方便。我们怎么办呢？

我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码(还是以百度翻译网站fanyi.baidu.com为例)：
    
```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    response = request.urlopen("http://www.fanyi.baidu.com/")
    html = response.read()
    html = html.decode("utf-8")
    print(html)

```

这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：

![](https://img-blog.csdn.net/20170228211300564?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？需要人为操作，且非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码的了。如下：

![](https://img-blog.csdn.net/20170228211321613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。

#### 2.2.2 自动获取网页编码方式的方法

获取网页编码的方式有很多，个人更喜欢用第三方库的方式。

首先我们需要安装第三方库chardet，它是用来判断编码的模块，安装方法如下图所示，只需要输入指令：

```
pip install chardet

```

![](https://img-blog.csdn.net/20170228211429052?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：

```
# -*- coding: UTF-8 -*-
from urllib import request
import chardet

if __name__ == "__main__":
    response = request.urlopen("http://fanyi.baidu.com/")
    html = response.read()
    charset = chardet.detect(html)
    print(charset)

```

运行程序，查看输出结果如下：

![](https://img-blog.csdn.net/20170228211514256?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

返回的是一个字典，这样我们就知道网页的编码方式了，通过获得的信息，采用不同的解码方式即可。

<br>
<br>
<br>
<br>
<br>
<br>



## reference:

1.[Python3网络爬虫(一)：利用urllib进行简单的网页抓取](https://blog.csdn.net/c406495762/article/details/58716886)




    
    