# 利用urllib进行简单的网页抓取

 <br>
 <br>
 <br>
 <br> 
 

## 1. 网络爬虫的定义

&emsp;&emsp;网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。举一个简单的例子，我们在浏览器的地址栏中输入的字符串就是URL，例如：https://www.baidu.com/

&emsp;&emsp;URL就是同意资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)：


```
protocol :// hostname[:port] / path / [;parameters][?query]#fragment
```

&emsp;&emsp;URL的格式由三部分组成：

&emsp;&emsp;(1)protocol：第一部分就是协议，例如百度使用的就是https协议；

&emsp;&emsp;(2)hostname[:port]：第二部分就是主机名(还有端口号为可选参数)，一般网站默认的端口号为80，例如百度的主机名就是www.baidu.com，这个就是服务器的地址;

&emsp;&emsp;(3)path：第三部分就是主机资源的具体地址，如目录和文件名等。

&emsp;&emsp;网络爬虫就是根据这个URL来获取网页信息的。

 <br>
 <br>

## 2. 简单爬虫实例


&emsp;&emsp;在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：

![](https://upload-images.jianshu.io/upload_images/10947003-02e42c6ec0bcc524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;1.urllib.request模块是用来打开和读取URLs的；

&emsp;&emsp;2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；

&emsp;&emsp;3.urllib.parse模块包含了一些解析URLs的方法；

&emsp;&emsp;4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。

 <br>

### 2.1 urlopen()打开网站

&emsp;&emsp;我们使用urllib.request.urlopen()这个接口函数就可以很轻松的打开一个网站，读取并打印信息。

![](https://upload-images.jianshu.io/upload_images/10947003-a71f6e29cda3c70d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。

&emsp;&emsp;了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：

```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    response = request.urlopen("http://fanyi.baidu.com")
    html = response.read()
    print(html)

```

&emsp;&emsp;urllib使用使用request.urlopen()打开和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()，将读到的信息打印出来。

&emsp;&emsp;运行程序ctrl+b，可以在Sublime中查看运行结果，如下：

![](https://upload-images.jianshu.io/upload_images/10947003-c1ffdd228b2b02ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;也可以在cmd(控制台)中输入指令：

```
python urllib_test01.py

```

&emsp;&emsp;运行py文件，输出信息是一样的，如下：

![](https://upload-images.jianshu.io/upload_images/10947003-4e82254155bd6003.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;其实这就是浏览器接收到的信息，只不过我们在使用浏览器的时候，浏览器已经将这些信息转化成了界面信息供我们浏览。当然这些代码我们也可以从浏览器中查看到。例如，使用谷歌浏览器，在任意界面单击右键选择检查，也就是审查元素(不是所有页面都可以审查元素的，例如起点中文网付费章节就不行.)，以百度界面为例，截图如下：

![](https://upload-images.jianshu.io/upload_images/10947003-2fbff4909ce52e67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)


 <br>

### 2.2 网页的信息进行解码

#### 2.2.1 通过简单的decode()命令将网页的信息进行解码

&emsp;&emsp;回归正题，虽然我们已经成功获取了信息，但是显然他们都是二进制的乱码，看起来很不方便。我们怎么办呢？

&emsp;&emsp;我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码(还是以百度翻译网站fanyi.baidu.com为例)：
    
```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    response = request.urlopen("http://www.fanyi.baidu.com/")
    html = response.read()
    html = html.decode("utf-8")
    print(html)

```

&emsp;&emsp;这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：

![](https://upload-images.jianshu.io/upload_images/10947003-caaf19a6dde4ad48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？需要人为操作，且非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码的了。如下：

![](https://upload-images.jianshu.io/upload_images/10947003-b9c364e5ba2e2a2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。

#### 2.2.2 自动获取网页编码方式的方法

&emsp;&emsp;获取网页编码的方式有很多，个人更喜欢用第三方库的方式。

&emsp;&emsp;首先我们需要安装第三方库chardet，它是用来判断编码的模块，安装方法如下图所示，只需要输入指令：

```
pip install chardet

```

![](https://upload-images.jianshu.io/upload_images/10947003-a85aa36eb7d349fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：

```
# -*- coding: UTF-8 -*-
from urllib import request
import chardet

if __name__ == "__main__":
    response = request.urlopen("http://fanyi.baidu.com/")
    html = response.read()
    charset = chardet.detect(html)
    print(charset)

```

&emsp;&emsp;运行程序，查看输出结果如下：

![](https://upload-images.jianshu.io/upload_images/10947003-b181bcecdb5e3703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;返回的是一个字典，这样我们就知道网页的编码方式了，通过获得的信息，采用不同的解码方式即可。



<br>
 <br>

## 3. 实例：利用urllib.urlopen向有道翻译发送数据获得翻译结果

&emsp;&emsp;详细看下urlopen的两个重要参数url和data，学习如何发送数据data

 <br>
 
### 3.1 urlopen的url参数 Agent

![](https://upload-images.jianshu.io/upload_images/10947003-36e73a42e54bae70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;url不仅可以是一个字符串，例如:http://www.baidu.com。url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：


```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    req = request.Request("http://fanyi.baidu.com/")
    response = request.urlopen(req)
    html = response.read()
    html = html.decode("utf-8")
    print(html)

```

&emsp;&emsp;同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。

&emsp;&emsp;urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。

![](https://upload-images.jianshu.io/upload_images/10947003-bfb0088c7062be4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;- geturl()返回的是一个url的字符串；

&emsp;&emsp;- info()返回的是一些meta标记的元信息，包括一些服务器的信息；

&emsp;&emsp;- getcode()返回的是HTTP的状态码，如果返回200表示请求成功。

&emsp;&emsp;关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。

![](https://upload-images.jianshu.io/upload_images/10947003-57894c4c3b489fe1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

![](https://upload-images.jianshu.io/upload_images/10947003-ae1e45cb12919bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：

```
# -*- coding: UTF-8 -*-
from urllib import request

if __name__ == "__main__":
    req = request.Request("http://fanyi.baidu.com/")
    response = request.urlopen(req)
    print("geturl打印信息：%s"%(response.geturl()))
    print('**********************************************')
    print("info打印信息：%s"%(response.info()))
    print('**********************************************')
    print("getcode打印信息：%s"%(response.getcode()))

```

&emsp;&emsp;可以得到如下运行结果：

![](https://upload-images.jianshu.io/upload_images/10947003-76a098a4bf0f49c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

 <br>
 
### 3.2 urlopen的data参数

&emsp;&emsp;我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：

&emsp;&emsp;从客户端向服务器提交数据使用POST；

&emsp;&emsp;从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。

&emsp;&emsp;如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。

&emsp;&emsp;data参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。

 <br>
 
### 3.3 发送data实例

&emsp;&emsp;向有道翻译发送data，得到翻译结果。

&emsp;&emsp;1. 打开有道翻译界面，如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-c701c6409237f9de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)


&emsp;&emsp;2. 鼠标右键检查，也就是审查元素，如下图所示：


![](https://upload-images.jianshu.io/upload_images/10947003-453205764a29e19b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)



&emsp;&emsp;3. 选择右侧出现的Network，如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-63df789fae74a517.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)


&emsp;&emsp;4. 在左侧输入翻译内容，输入Jack，如下图所示：


![](https://upload-images.jianshu.io/upload_images/10947003-0fe6383f115dcf58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)


&emsp;&emsp;5. 点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-aee2bfb3cb3c83c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;6. 点击上图红框中的内容，查看它的信息，如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-910e793b6a959845.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

![](https://upload-images.jianshu.io/upload_images/10947003-2c54a433900a5fc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;7. 记住这些信息，这是我们一会儿写程序需要用到的。

&emsp;&emsp;新建文件translate_test.py，编写如下代码：

```
# -*- coding: UTF-8 -*-
from urllib import request
from urllib import parse
import json

if __name__ == "__main__":
    #对应上图的Request URL
    Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=https://www.baidu.com/link'
    #创建Form_Data字典，存储上图的Form Data
    Form_Data = {}
    Form_Data['type'] = 'AUTO'
    Form_Data['i'] = 'Jack'
    Form_Data['doctype'] = 'json'
    Form_Data['xmlVersion'] = '1.8'
    Form_Data['keyfrom'] = 'fanyi.web'
    Form_Data['ue'] = 'ue:UTF-8'
    Form_Data['action'] = 'FY_BY_CLICKBUTTON'
    #使用urlencode方法转换标准格式
    data = parse.urlencode(Form_Data).encode('utf-8')
    #传递Request对象和转换完格式的数据
    response = request.urlopen(Request_URL,data)
    #读取信息并解码
    html = response.read().decode('utf-8')
    #使用JSON
    translate_results = json.loads(html)
    #找到翻译结果
    translate_results = translate_results['translateResult'][0][0]['tgt']
    #打印翻译信息
    print("翻译的结果是：%s" % translate_results)
```

&emsp;&emsp;这样我们就可以查看翻译的结果了，如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-71571a0142b002c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;JSON是一种轻量级的数据交换格式，我们需要从爬取到的内容中找到JSON格式的数据，这里面保存着我们想要的翻译结果，再将得到的JSON格式的翻译结果进行解析，得到我们最终想要的样子：杰克。

<br>
<br>

## 4. urllib.error异常

<br>

### 4.1 urllib.error介绍

&emsp;&emsp;urllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：

![](https://upload-images.jianshu.io/upload_images/10947003-f5866750ec54f943.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)


&emsp;&emsp;URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。

1. URLError

&emsp;&emsp;让我们先看下URLError的异常，创建文件urllib_test06.py，编写如下代码：

```
# -*- coding: UTF-8 -*-
from urllib import request
from urllib import error

if __name__ == "__main__":
    #一个不存在的连接
    url = "http://www.iloveyou.com/"
    req = request.Request(url)
    try:
        response = request.urlopen(req)
        html = response.read().decode('utf-8')
        print(html)
    except error.URLError as e:
        print(e.reason)

```

&emsp;&emsp;我们可以看到如下运行结果：

![](https://upload-images.jianshu.io/upload_images/10947003-4c733cb5ff16bd52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

2. HTTPError

&emsp;&emsp;再看下HTTPError异常，创建文件urllib_test07.py，编写如下代码：

```
# -*- coding: UTF-8 -*-
from urllib import request
from urllib import error

if __name__ == "__main__":
    #一个不存在的连接
    url = "http://www.douyu.com/Jack_Cui.html"
    req = request.Request(url)
    try:
        responese = request.urlopen(req)
        # html = responese.read()
    except error.HTTPError as e:
        print(e.code)
```

&emsp;&emsp;运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com这个服务器是存在的，但是我们要查找的Jack_Cui.html资源是没有的，所以抛出404异常。

![](https://upload-images.jianshu.io/upload_images/10947003-3065682f75be0506.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

<br>

### 4.2 URLError和HTTPError混合使用

&emsp;&emsp;最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。

![](https://upload-images.jianshu.io/upload_images/10947003-e0ce6cbdaf7382da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test08.py，编写代码如下：

```
# -*- coding: UTF-8 -*-
from urllib import request
from urllib import error

if __name__ == "__main__":
    #一个不存在的连接
    url = "http://www.douyu.com/Jack_Cui.html"
    req = request.Request(url)
    try:
        responese = request.urlopen(req)
    except error.URLError as e:
        if hasattr(e, 'code')
            print("HTTPError")
            print(e.code)
        elif hasattr(e, 'reason')
            print("URLError")
            print(e.reason)

```

&emsp;&emsp;运行结果如下：

![](https://upload-images.jianshu.io/upload_images/10947003-b978625f9a3ced1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)









<br>
<br>
<br>
<br>
<br>



## reference:

1. [Python3网络爬虫(一)：利用urllib进行简单的网页抓取](https://blog.csdn.net/c406495762/article/details/58716886)

2. [利用urllib.urlopen向有道翻译发送数据获得翻译结果](https://blog.csdn.net/c406495762/article/details/59095864)

3. [Python3网络爬虫(三)：urllib.error异常](https://blog.csdn.net/c406495762/article/details/59488464)


    
    