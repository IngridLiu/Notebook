# Spark RDD(Resilient Distributed Datasets,RDD)
 
<br>
<br>

## 1. RDD基础

Spark中的RDD就是一个不可变的分布式对象集合。可以使用两种方法创建RDD：读取一个外部数据集或在驱动器程序里分发驱动器程序中的对象集合。创建出来后，RDD支持两种类型的操作：转化操作（transformation）和行动操作（action），转化操作会生成一个新的RDD，行动操作会对RDD计算出一个结果，这个结果可以返回到驱动器中或者储存到存储系统中。RDD只有第一次在一个行动操作中用到时，才会真正计算。如果想在多个行动操作中重用同一个RDD，可以使用RDD.persist()让spark把这个RDD缓存下来。

```bash
scala> pythonLines.persist()    # 把RDD持久化到内存中
scala> pythonLines.count()
scala> pythonLines.first()
```

总的来说，每个Spark程序或shell会话都会按如下方式工作:

(1) 从外部数据创建出输入RDD;

(2) 使用诸如filter()这样的转化操作对RDD进行转化，以定义新的RDD:

(3) 告诉Spark对需要被重用的中间结果RDD执行persist()操作;

(4) 使用行动操作（例如count()和first()等）来触发一次并行计算，Spark会对计算进行优化后再执行。

具体操作如下：

```bash
# RDD创建
# 创建RDD的两种方法：读取外部数据集以及在一驱动器中对一个集合进行并行化；
val lines = sc.textFile(path)
val lines = sc.parallelize(List["pandas", "i like pandas"])

# 转化操作
# 针对各个元素的转化操作
rdd.filter(function)    # 返回一个由通过传给filter()的函数的元素组成新的RDD
rdd.map(function)   # 将函数应用于RDD中的每一个元素，将返回值构成新的RDD
rdd.flatmap(x => function)   # 将function应用到rdd中的每一个元素，将返回的迭代器的所有内容构成新的RDD；通常用来切分单词；val words = lines.flatMap(line => line.split(" "))
rdd.dictinct()  # 去重
rdd.sample()
# 伪集合操作
rdd1.union(rdd2)    # 合并rdd1和rdd2，不去重
rdd1.intersection(rdd2) # 获取rdd1和rdd2均含有的元素，且去重
rdd1.subtract(rdd2) # 移除rdd1中与rdd2相同的内容
rdd1.cartesian(rdd2)    # 获取rdd1和rdd2元素的笛卡尔积



# 行动操作
rdd.take()
rdd.foreach(function)
rdd.collect()


```