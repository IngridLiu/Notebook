# Spark RDD(Resilient Distributed Datasets,RDD)
 
<br>
<br>

## 1. RDD基础

Spark中的RDD就是一个不可变的分布式对象集合。可以使用两种方法创建RDD：读取一个外部数据集或在驱动器程序里分发驱动器程序中的对象集合。创建出来后，RDD支持两种类型的操作：转化操作（transformation）和行动操作（action），转化操作会生成一个新的RDD，行动操作会对RDD计算出一个结果，这个结果可以返回到驱动器中或者储存到存储系统中。RDD只有第一次在一个行动操作中用到时，才会真正计算。如果想在多个行动操作中重用同一个RDD，可以使用RDD.persist()让spark把这个RDD缓存下来。

```bash
scala> pythonLines.persist()    # 把RDD持久化到内存中
scala> pythonLines.count()
scala> pythonLines.first()
```

总的来说，每个Spark程序或shell会话都会按如下方式工作:

(1) 从外部数据创建出输入RDD;

(2) 使用诸如filter()这样的转化操作对RDD进行转化，以定义新的RDD:

(3) 告诉Spark对需要被重用的中间结果RDD执行persist()操作;

(4) 使用行动操作（例如count()和first()等）来触发一次并行计算，Spark会对计算进行优化后再执行。

具体操作如下：

```scala
// RDD创建
// 创建RDD的两种方法：读取外部数据集以及在一驱动器中对一个集合进行并行化；
val lines = sc.textFile(path)
val lines = sc.parallelize(List["pandas", "i like pandas"])

// 转化操作
// 针对各个元素的转化操作
rdd.filter(function)    # 返回一个由通过传给filter()的函数的元素组成新的RDD
rdd.map(function)   # 将函数应用于RDD中的每一个元素，将返回值构成新的RDD
rdd.flatmap(x => function)   # 将function应用到rdd中的每一个元素，将返回的迭代器的所有内容构成新的RDD；通常用来切分单词；val words = lines.flatMap(line => line.split(" "))
rdd.dictinct()  # 去重
rdd.sample()
// 伪集合操作
rdd1.union(rdd2)    # 合并rdd1和rdd2，不去重
rdd1.intersection(rdd2) # 获取rdd1和rdd2均含有的元素，且去重
rdd1.subtract(rdd2) # 移除rdd1中与rdd2相同的内容
rdd1.cartesian(rdd2)    # 获取rdd1和rdd2元素的笛卡尔积

// 行动操作
rdd.reduce((x, y) => x+y)   # 对rdd中元素进行加和操作;reduce()接收一个函数作为参数，这个函数要操作两个相同元素类型的RDD数据并返回一个同样类型的新元素。
rdd.fold(zero)(function)    # 和reduce()一样，但是需要提供初始值zero
rdd.collect()   # 用于获取整个RDD中的数据。只有当整个数据集能在单台机器的内存中放得下时，才能使用collect()，因此，collect()不能用在大规模数据集上。
rdd.count() # 计算rdd中元素个数，可用于测试数据。
rdd.countByBalue()  # 计算rdd中各元素出现的次数
rdd.take(n) # 从rdd中返回n个元素
rdd.top(n)  # 从rdd中返回最前面的n个元素
rdd.takeOrdered(n)(function)    # 从rdd中按照提供的顺序返回最前面的n个元素
rdd.takeSample(withPlacement, n, [seed])    # 从rdd中任意返回n个元素
rdd.foreach()   # 对rdd中的每个元素使用给定的函数。
rdd.aggregate() # 和reduce()一样，但是需要返回不同类型的函数；使用aggregate()需要提供我们期待返回的类型的初始值，然后通过一个函数把rdd中的元素合并起来放入累加器，考虑到每个节点是在本地进行累加的，最终还需要提供第二个函数来将累加器两两合并。

// example:求均值
val result = rdd.aggregate((0,0))(
    (acc, value) => (acc._1 + value, acc._2 + 1),
    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
val avg = result._1 + result._2.todouble

```

## 2. 