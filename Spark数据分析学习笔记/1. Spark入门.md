# Spark入门


<br>
<br>

&emsp;&emsp;Spark 是一个用来实现快速而通用的集群计算的平台。Spark是内存计算引擎,支持scala开发，支持java scala python开发接口。

## 1. Spark介绍

### 1.1 Spark组件介绍

&emsp;&emsp;Spark的各个组件如图所示：

![图1:Spark软件栈](https://upload-images.jianshu.io/upload_images/10947003-7d265dd5ff127f8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)

&emsp;&emsp;Spark Core 实现了Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core 中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD）的API 定义。RDD 表示分布在多个计算节点上可以并行操作的元素集合，是Spark 主要的编程抽象。Spark Core 提供了创建和操作这些集合的多个API。

&emsp;&emsp;Spark SQL 是Spark 用来操作结构化数据的程序包。通过Spark SQL，我们可以使用SQL或者Apache Hive 版本的SQL 方言（HQL）来查询数据。Spark SQL 支持多种数据源，比如Hive 表、Parquet 以及JSON 等。除了为Spark 提供了一个SQL 接口，Spark SQL 还支持开发者将SQL 和传统的RDD 编程的数据操作方式相结合.

&emsp;&emsp;Spark Streaming 是Spark 提供的对实时数据进行流式计算的组件。

&emsp;&emsp;Spark 中还包含一个提供常见的机器学习（ML）功能的程序库，叫作MLlib。MLlib 提供了很多种机器学习算法，包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。

&emsp;&emsp;GraphX 是用来操作图（比如社交网络的朋友关系图）的程序库，可以进行并行的图计算。

<br>

### 1.2 Spark 运行模式

- local模式：

- standalone模式：

- yarn模式：spark用yarn做资源管理和分配。

- mesos模式：spark用mesos替代yarn做资源管理和分配。

Spark shell 可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由Spark 自动控制完成。

<br>

## 2. Spark核心基本概念介绍

&emsp;&emsp;从上层来看，每个Spark应用都由一个驱动器程序（driver program）来发起集群上的各种并行操作。驱动器程序通过一个SparkContext对象来访问Spark。这个对象代表对计算集群的一个连接。shell 启动时已经自动创建了一个SparkContext 对象，是一个叫作sc 的变量。驱动器程序一般要管理多个执行器节点。

```bash
bash$ spark-shell #进入spark shell, 按Ctrl+D可退出spark shell

scala> val lines = sc.textFile("README.md") # 创建一个叫lines的RDD。shell 启动时已经自动创建了一个SparkContext 对象，是一个叫作sc 的变量。
scala> lines.count()    # 返回总行数
scala> lines.first()    # 返回第一行的内容

scala> val pythonLines = lines.filter(line => line.contains("Python"))  //pythonLines: spark.RDD[String] = FilteredRDD[...]
scala> pythonLines.first()  //res0: String = ## Interactive Python Shell
```

&emsp;&emsp;在scala中初始化Spark

```scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContest._

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)
```

&emsp;&emsp;以上例子展示了创建SparkContext 的最基本的方法，你只需传递两个参数：

&emsp;&emsp;• 集群URL：告诉Spark如何连接到集群上。在这几个例子中我们使用的是local，这个特殊值可以让Spark 运行在单机单线程上而无需连接到集群。

&emsp;&emsp;• 应用名：在例子中我们使用的是My App。当连接到一个集群时，这个值可以帮助你在集群管理器的用户界面中找到你的应用。

&emsp;&emsp;最后，关闭Spark可以调用SparkContext的stop()方法，或者直接退出应用（比如通过System.exit(0) 或者sys.exit()）。

小例子：Scala 版本的单词数统计应用

```scala
// 创建一个scala版本的SparkContext
val conf = new SparkConf().setAppName("wordCount")
val sc = new SparkContext(conf)
// 读取我们的输入数据
val input = sc.textFile(inputFile)
// 把它切分呈一个个单词
val words = input.flatMap(line => line.split(" "))
val counts = words.map(word => (word, 1)).reduceByKey{case (x, y) => x + y}
// 将统计出来的单词总数存入一个文本文件，引发求值
counts.saveAsTextFile(outputFile)
```


<br>
<br>
<br>
<br>



## Reference:

1. 《Spark快速大数据分析》