# Spark DataFrame学习笔记


<br>
<br>

## 1. DataFrame中的函数

```scala
// DataFrame 的函数

// 创建DataFrame
strs.split(",").toSeq.toDF("column_name")   # 将str数据转为DataFrame

// Action 操作
collect()   //返回值是一个数组，返回dataframe集合所有的行
collectAsList() //返回值是一个java类型的数组，返回dataframe集合所有的行
count() //返回一个number类型的，返回dataframe集合的行数
describe(cols: String*) //返回一个通过数学计算的类表值(count, mean, stddev, min, and max)，这个可以传多个参数，中间用逗号分隔，如果有字段为空，那么不参与运算，只这对数值类型的字段。例如df.describe("age", "height").show()
first() //返回第一行 ，类型是row类型
head() //返回第一行 ，类型是row类型
head(n:Int) //返回n行  ，类型是row 类型
show()  //返回dataframe集合的值 默认是20行，返回类型是unit
show(n:Int) //返回n行，，返回值类型是unit
table(n:Int) //返回n行  ，类型是row 类型

// dataframe的基本操作
cache() //同步数据的内存
columns //返回一个string类型的数组，返回值是所有列的名字
dtypes  //返回一个string类型的二维数组，返回值是所有列的名字以及类型
explan()    //打印执行计划  物理的
explain(n:Boolean)  //输入值为 false 或者true ，返回值是unit  默认是false ，如果输入true 将会打印 逻辑的和物理的
isLocal 返回值是Boolean类型，如果允许模式是local返回true 否则返回false
persist(newlevel:StorageLevel) 返回一个dataframe.this.type 输入存储模型类型
printSchema() 打印出字段名称和类型 按照树状结构来打印
registerTempTable(tablename:String) 返回Unit ，将df的对象只放在一张表里面，这个表随着对象的删除而删除了
schema 返回structType 类型，将字段名称和类型按照结构体类型返回
toDF()返回一个新的dataframe类型的
toDF(colnames：String*)将参数中的几个字段返回一个新的dataframe类型的，
unpersist() 返回dataframe.this.type 类型，去除模式中的数据
unpersist(blocking:Boolean)返回dataframe.this.type类型 true 和unpersist是一样的作用false 是去除RDD

集成查询：
agg(expers:column*) 返回dataframe类型 ，同数学计算求值
df.agg(max("age"), avg("salary"))
df.groupBy().agg(max("age"), avg("salary"))
agg(exprs: Map[String, String])  返回dataframe类型 ，同数学计算求值 map类型的
df.agg(Map("age" -> "max", "salary" -> "avg"))
df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
agg(aggExpr: (String, String), aggExprs: (String, String)*)  返回dataframe类型 ，同数学计算求值
df.agg(Map("age" -> "max", "salary" -> "avg"))
df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
apply(colName: String) 返回column类型，捕获输入进去列的对象
as(alias: String) 返回一个新的dataframe类型，就是原来的一个别名
col(colName: String)  返回column类型，捕获输入进去列的对象
cube(col1: String, cols: String*) 返回一个GroupedData类型，根据某些字段来汇总
distinct 去重 返回一个dataframe类型
drop(col: Column) 删除某列 返回dataframe类型
dropDuplicates(colNames: Array[String]) 删除相同的列 返回一个dataframe
except(other: DataFrame) 返回一个dataframe，返回在当前集合存在的在其他集合不存在的
explode[A, B](inputColumn: String, outputColumn: String)(f: (A) ⇒ TraversableOnce[B])(implicit arg0: scala.reflect.api.JavaUniverse.TypeTag[B]) 返回值是dataframe类型，这个 将一个字段进行更多行的拆分
df.explode("name","names") {name :String=> name.split(" ")}.show();
将name字段根据空格来拆分，拆分的字段放在names里面
filter(conditionExpr: String): 刷选部分数据，返回dataframe类型 df.filter("age>10").show();  df.filter(df("age")>10).show();   df.where(df("age")>10).show(); 都可以
groupBy(col1: String, cols: String*) 根据某写字段来汇总返回groupedate类型   df.groupBy("age").agg(Map("age" ->"count")).show();df.groupBy("age").avg().show();都可以
intersect(other: DataFrame) 返回一个dataframe，在2个dataframe都存在的元素
join(right: DataFrame, joinExprs: Column, joinType: String)
// 一个是关联的dataframe，第二个关联的条件，第三个关联的类型：inner, outer, left_outer, right_outer, leftsemi
df.join(ds,df("name")===ds("name") and  df("age")===ds("age"),"outer").show();
limit(n: Int) 返回dataframe类型  去n 条数据出来
na: DataFrameNaFunctions ，可以调用dataframenafunctions的功能区做过滤 df.na.drop().show(); 删除为空的行
orderBy(sortExprs: Column*) 做alise排序
select(cols:string*) dataframe 做字段的刷选 df.select($"colA", $"colB" + 1)
selectExpr(exprs: String*) 做字段的刷选 df.selectExpr("name","name as names","upper(name)","age+1").show();
sort(sortExprs: Column*) 排序 df.sort(df("age").desc).show(); 默认是asc
unionAll(other:Dataframe) 合并 df.unionAll(ds).show();
withColumnRenamed(existingName: String, newName: String) 修改列表 df.withColumnRenamed("name","names").show();
withColumn(colName: String, col: Column) 增加一列 df.withColumn("aa",df("name")).show();```
```

#### 补充说明：

**8. df.show()**

&emsp;&emsp;显示DataFrame中的值，结果和show table一样；

**13. df.filter()**

&emsp;&emsp;对DataFrame中的值做过滤；

```scala
// 逻辑运算符：>, <, ===
df.filter($"num" === num)
df.filter($"num" > num)
df.filter($"num" < num)

df.filter("num=2")
df.filter("num>2")
df.filter("num<2")

// 对字符串过滤
df.filter($'id'.equalTo('a'))

// 多条件判断
df.filter($"num" === 2 && $"id".equalTo("a"))
df.filter($"num" === 1 || "num"===3)
```

**16. df.join()**

&emsp;&emsp;对两个DataFrame做join操作；

```scala
// join 是DataFrame的内置函数，
// params: right是要join的DataFrame
//         usingColumns是Seq[String]类型,表示根据这些列进行join
//         jointype表示join的方式，joinType可以是”inner”、“left”、“right”、“full”分别对应inner join, left join, right join, full join，默认值是”inner”，代表内连接。
// return: DataFrame

def join(right : DataFrame, usingColumns : Seq[String], joinType : String) : DataFrame 
def join(right : DataFrame, joinExprs : Column, joinType : String) : DataFrame 

df1.join(df2, Seq("num","id"), "right")

```


**25. df.withColumn()**

&emsp;&emsp;在df中选择一列，对其进行操作

```scala
// 对df中的indicator列做操作，利用正则化用"retention"替换掉列值的"dau"字符串;"dau"可以用正则化表达式进行匹配;
df.withColumn("indicator", regexp_replace(col("indicator"), "dau", "retention")).
```

<br>
<br>
<br>
<br>

## Reference:

1.[sparkSQL:dataframe](https://blog.csdn.net/zhonglongshen/article/details/52386251)

2.[Apache Spark DataFrames入门指南：操作DataFrame](https://www.iteblog.com/archives/1566.html)