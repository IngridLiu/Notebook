# 常见相似度度量

<br>
<br>
<br>
<br>

本文目录：

1. 欧几里得距离相似度

2. 曼哈顿距离

3. 切比雪夫距离(Chebyshev Distance)

4. 闵可夫斯基距离(Minkowski Distance)

5. 标准化欧氏距离 (Standardized Euclidean distance )

6. 马氏距离(Mahalanobis Distance)

7. 余弦相似度

8. 汉明距离(Hamming distance)

9. 杰卡德相似系数(Jaccard similarity coefficient)

10. 皮尔逊相关系数(Pearson product-moment correlation coefficient)

<br>
<br>

## 1 欧几里得距离相似度：

&emsp;&emsp;定义在两个向量（两个点）上：点x和点y的欧式距离为：

![欧几里得距离相似度](https://upload-images.jianshu.io/upload_images/10947003-803e01842a30a401.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&emsp;&emsp;常利用欧几里得距离描述相似度时，需要取倒数归一化，sim = 1.0/(1.0+distance)，利用numpy实现如下：

**python实现欧式距离**

```python
# 计算欧式距离
distance = np.linalg.norm(data_np[i]-data_np[j])
# 归一化
sim[i][j] = 1.0 / (1.0 + dis)
```

## 2 曼哈顿距离

&emsp;&emsp;从名字就可以猜出这种距离的计算方法了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为城市街区距离(City Block distance)。

&emsp;&emsp;(1)二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离

![](https://pic002.cnblogs.com/images/2011/63234/2011030823213652.png)

&emsp;&emsp;(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离

![](https://pic002.cnblogs.com/images/2011/63234/2011030823231354.png)

&emsp;&emsp;**python实现曼哈顿距离：**

```python
distance = np.sum(np.abs(data_np[i] - data_np[j]))  # 方法一，采用numpy直接计算

distance = np.linalg.norm(data_np[i] - data_np[j], ord = 1) # np.linalg.norm(x, ord=None, axis=None, keepdims=False), ord表示范数类型，axis表示处理维度，keepding：是否保持矩阵的二维特性；
```

<br>

## 3 切比雪夫距离(Chebyshev Distance)

&emsp;&emsp;国际象棋玩过么？国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？自己走走试试。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。

&emsp;&emsp;(1)二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离

![](https://pic002.cnblogs.com/images/2011/63234/2011030823234117.png)

&emsp;&emsp;(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离

![](https://pic002.cnblogs.com/images/2011/63234/2011030823235870.png)

&emsp;&emsp;**python实现切比雪夫距离：**

```python
distance = np.abs(data_np[i] - data_np[j]).max()    #方法一，采用numpy直接计算
distance = np.linalgnorm(data_np[i]- data_np[j], ord = np.inf)  # 方法二，np.linalgnorm,np.inf指无穷大；
```

<br>

## 4 闵可夫斯基距离(Minkowski Distance)

&emsp;&emsp;闵氏距离不是一种距离，而是一组距离的定义。

### 4.1 闵氏距离的定义

&emsp;&emsp;两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：

![](https://img-blog.csdn.net/20171108154351059?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTk3MDc1MjE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

&emsp;&emsp;其中p是一个变参数。

&emsp;&emsp;当p=1时，就是曼哈顿距离

&emsp;&emsp;当p=2时，就是欧氏距离

&emsp;&emsp;当p→∞时，就是切比雪夫距离

&emsp;&emsp;根据变参数的不同，闵氏距离可以表示一类的距离。


### 4.2 闵氏距离的缺点

&emsp;&emsp;闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。

&emsp;&emsp;举个例子：二维样本(身高,体重)，其中身高范围是150~190，体重范围是50~60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。

&emsp;&emsp;简单说来，闵氏距离的缺点主要有两个：

&emsp;&emsp;(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。

&emsp;&emsp;(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。

### 4.3 python实现闵氏距离：

```python
distance = np.linalg.norm(np_data[i] - data_np[j], ord = p) # ord表示范数类型
```
<br>

## 5 标准化欧氏距离 (Standardized Euclidean distance )

&emsp;&emsp;标准欧氏距离的定义

&emsp;&emsp;标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：

&emsp;&emsp;而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823264688.png)

&emsp;&emsp;标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差

&emsp;&emsp;经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823272054.png)

&emsp;&emsp;如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。

<br>

## 6 马氏距离(Mahalanobis Distance)

### 6.1 马氏距离定义

&emsp;&emsp;有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823274286.png)

&emsp;&emsp;而其中向量Xi与Xj之间的马氏距离定义为：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823280193.png)

&emsp;&emsp;若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823281650.png)

&emsp;&emsp;也就是欧氏距离了。

&emsp;&emsp;若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。

### 6.2 马氏距离的优缺点

&emsp;&emsp;马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。


<br>

## 7 余弦相似度

&emsp;&emsp;几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。

&emsp;&emsp;在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823283429.png)

&emsp;&emsp;两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦

&emsp;&emsp;类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。

![](https://pic002.cnblogs.com/images/2011/63234/2011030823293892.png)

&emsp;&emsp;即：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823294588.png)
 
&emsp;&emsp;夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。

**python实现余弦相似度：**

```python
num = np.dot(data_np[i], data_np[j]) #若为行向量则 A * B 
denom = linalg.norm(A) * linalg.norm(B)  
cos = num / denom #余弦值  
sim = 0.5 + 0.5 * cos #归一化 
```

<br>

## 8 汉明距离(Hamming distance)

&emsp;&emsp;两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。

&emsp;&emsp;应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。

**python实现汉明距离：**

```python
smstr=np.nonzero(np_data[i] - np_data[j])
sim= np.shape(smstr[0])[0]
```

<br>

## 9 杰卡德相似系数(Jaccard similarity coefficient)

### 9.1 杰卡德相似系数

&emsp;&emsp;两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。
       
![](https://pic002.cnblogs.com/images/2011/63234/2011030823303566.png)

&emsp;&emsp;杰卡德相似系数是衡量两个集合的相似度一种指标。

### 9.2 杰卡德距离

&emsp;&emsp;与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823310119.png)

&emsp;&emsp;杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。

### 9.3 杰卡德相似系数与杰卡德距离的应用

&emsp;&emsp;可将杰卡德相似系数用在衡量样本的相似度上。

&emsp;&emsp;样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。

&emsp;&emsp;p ：样本A与B都是1的维度的个数

&emsp;&emsp;q ：样本A是1，样本B是0的维度的个数

&emsp;&emsp;r ：样本A是0，样本B是1的维度的个数

&emsp;&emsp;s ：样本A与B都是0的维度的个数

&emsp;&emsp;这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。

&emsp;&emsp;而样本A与B的杰卡德距离表示为：

![](https://pic002.cnblogs.com/images/2011/63234/2011030823313638.png)

```python
import scipy.spatial.distance as dist

matv=np.array([data_np[i],data_np[j]])
dis=dist.pdist(matv,'jaccard')
```

<br>

### 10 皮尔逊相关系数(Pearson product-moment correlation coefficient)

&emsp;&emsp;皮尔逊相关系数即为相关系数 ( Correlation coefficient )与相关距离(Correlation distance)

&emsp;&emsp;相关系数的定义

![](https://pic002.cnblogs.com/images/2011/63234/2011030823322444.png)

&emsp;&emsp;相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。

![](https://pic002.cnblogs.com/images/2011/63234/2011030823323390.png)

&emsp;&emsp;相关系数的分类

&emsp;&emsp;0.8-1.0 极强相关

&emsp;&emsp;0.6-0.8 强相关

&emsp;&emsp;0.4-0.6 中等程度相关

&emsp;&emsp;0.2-0.4 弱相关

&emsp;&emsp;0.0-0.2 极弱相关或无相关

<br>
<br>
<br>
<br>


## Reference:

1.[机器学习中的相似性度量](http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html)

2.[推荐算法入门（1）相似度计算方法大全](https://zhuanlan.zhihu.com/p/33164335)

3.[Python Numpy计算各类距离](https://blog.csdn.net/qq_19707521/article/details/78479532)

4.[皮尔逊积矩相关系数](https://zh.wikipedia.org/wiki/皮尔逊积矩相关系数)