## 自然语言处理基础



文本分类的处理大致分为文本预处理、文本特征提取、分类模型构建等。和英文文本处理分类相比，中文文本的预处理是关键技术。

## 1. 中文分词：

针对中文文本分类时，很关键的一个技术就是中文分词。特征粒度为词粒度远远好于字粒度，其大部分分类算法不考虑词序信息，基于字粒度的损失了过多的n-gram信息。下面简单总结一下中文分词技术:基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法 [1]。

### 1.1 基于字符串匹配的分词方法：

过程：这是一种基于词典的中文分词，核心是首先建立统一的词典表，当需要对一个句子进行分词时，首先将句子拆分成多个部分，将每一个部分与字典一一对应，如果该词语在词典中，分词成功，否则继续拆分匹配直到成功。

核心： 字典，切分规则和匹配顺序是核心。

分析：优点是速度快，时间复杂度可以保持在O（n）,实现简单，效果尚可；但对歧义和未登录词处理效果不佳。

### 1.2 基于理解的分词方法：

基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。

### 1.3 基于统计的分词方法：

过程：统计学认为分词是一个概率最大化问题，即拆分句子，基于语料库，统计相邻的字组成的词语出现的概率，相邻的词出现的次数多，就出现的概率大，按照概率值进行分词，所以一个完整的语料库很重要。

主要的统计模型有： N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）等。

<br>

## 2. 文本预处理

### 2.1 分词

中文任务分词必不可少，一般使用jieba分词，工业界的翘楚。

### 2.2 去停用词

建立停用词字典，目前停用词字典有2000个左右，停用词主要包括一些副词、形容词及其一些连接词。通过维护一个停用词表，实际上是一个特征提取的过程，本质 上是特征选择的一部分。

### 2.3 词性标注

在分词后判断词性（动词、名词、形容词、副词…），在使用jieba分词的时候设置参数就能获取。

<br>

## 3. 文本特征工程

文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。

### 3.1 基于词袋模型的特征表示：

以词为单位（Unigram）构建的词袋可能就达到几万维，如果考虑二元词组（Bigram）、三元词组（Trigram）的话词袋大小可能会有几十万之多，因此基于词袋模型的特征表示通常是极其稀疏的。

（1）词袋特征的方法有三种：

Naive版本：不考虑词出现的频率，只要出现过就在相应的位置标1，否则为0；

考虑词频（即term frequency）：，认为一段文本中出现越多的词越重要，因此权重也越大；

考虑词的重要性：以TF-IDF表征一个词的重要程度。TF-IDF反映了一种折中的思想：即在一篇文档中，TF认为一个词出现的次数越大可能越重要，但也可能并不是（比如停用词：“的”“是”之类的）；IDF认为一个词出现在的文档数越少越重要，但也可能不是（比如一些无意义的生僻词）。

（2）优缺点：

优点： 词袋模型比较简单直观，它通常能学习出一些关键词和类别之间的映射关系

缺点： 丢失了文本中词出现的先后顺序信息；仅将词语符号化，没有考虑词之间的语义联系（比如，“麦克风”和“话筒”是不同的词，但是语义是相同的）；

### 3.2 基于embedding的特征表示： 

通过词向量计算文本的特征。（主要针对短文本）

取平均： 取短文本的各个词向量之和（或者取平均）作为文本的向量表示；

网络特征： 用一个pre-train好的NN model得到文本作为输入的最后一层向量表示；

### 3.3 基于NN Model抽取的特征： 

NN的好处在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力。

### 3.4 基于任务本身抽取的特征：

主要是针对具体任务而设计的，通过我们对数据的观察和感知，也许能够发现一些可能有用的特征。有时候，这些手工特征对最后的分类效果提升很大。举个例子，比如对于正负面评论分类任务，对于负面评论，包含负面词的数量就是一维很强的特征。

### 3.5 特征融合：

对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如比较流行的GDBT, XGBoost）；对于特征维数较低、数据模式简单的情况，建议用简单的线性模型即可（如LR）。

### 3.6 主题特征：

LDA（文档的话题）： 可以假设文档集有T个话题，一篇文档可能属于一个或多个话题，通过LDA模型可以计算出文档属于某个话题的概率，这样可以计算出一个DxT的矩阵。LDA特征在文档打标签等任务上表现很好。

LSI（文档的潜在语义）： 通过分解文档-词频矩阵来计算文档的潜在语义，和LDA有一点相似，都是文档的潜在特征。




<br>
<br>
<br>
<br>

## Reference:

1. [文本分类概述（nlp）](https://blog.csdn.net/u014248127/article/details/80774668)

