# 词向量模型-cw2vec

<br>
<br>

## 1. 背景介绍

目前已经存在很多的词向量模型，但是较多的词向量模型都是基于西方语言，像英语，西班牙语，德语等，这些西方语言的内部组成都是拉丁字母，然而，由于中文书写和西方语言完全不同，中文词语包含很少的中文字符，但是中文字符内部包含了很强的语义信息，因此，如何有效利用中文字符内部的语义信息来训练词向量，成为近些年研究的热点。

通过观察中文字符内部组成，发现中文字符包含偏旁部首、字符组件，笔画信息等语义信息特征（如下图），基于偏旁部首和汉字组件特征的中文词向量模型已经有人提出，并取得了较好的效果。

本篇论文采用笔画信息作为特征，由于每个字符包含很多的笔画，类似于一个英文单词包含很多的拉丁字母，在这个基础之上，提出了笔画的n-gram特征。这个思想来源于2016年facebook提出的论文（Enriching Word Vectors with Subword Information），目前facebook这篇论文已经被引用300多次，影响力很大，cw2vec可以称之为中文版本的fasttext。

<br>

## 2. 模型介绍

(1) 词语分割

把中文词语分割为单个字符，为了获取中文字符的笔画信息。

>>> 词语：大人 分割为：（1）大 （2）人

(2) 笔画特征

获取中文字符的笔画信息，并且把字符的笔画信息合并，得到词语的笔画信息。

>>> 大： 一ノ丶
>>> 人： ノ丶
>>> 大人： 一ノ丶 ノ丶

(3) 笔画特征数字化

为了方便，论文提及把笔画信息数字化，用数字代表每一种笔画信息，如下图。

那么“大人”这个词的笔画信息就可以表示为：

>>> 大人： 一ノ丶 ノ丶
>>> 大人：13434

我从训练语料中获取到13354个汉字，并获取笔画信息，统计笔画种类和上图一致，只有5种笔画信息。

(4) N元笔画特征

提取词语笔画信息的n-gram特征。

>>> 3-gram：134、343、434
>>> 4-gram：1343、3434
>>> 5-gram：13434
>>> ......

(5) cw2vec模型

word2vec提出了CBOW和Skip-Gram两个模型（详解），cw2vec在Skip-Gram基础之上进行改进，把词语的n-gram笔画特征信息代替词语进行训练，cw2vec模型如下图。

>>> 短语：治理 雾霾 刻不容缓
>>> 中心词：雾霾
>>> 上下文词：治理，刻不容缓

![](https://img-blog.csdn.net/20180818114952322?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hIVE5BTg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

论文中提及上下文词向量（context word embedding）为最终cw2vec模型的输出词向量。

<br>
<br>
<br>
<br>

## Reference:

1. [自然语言处理算法之cw2vec理论及其实现（基于汉字笔画）](https://blog.csdn.net/HHTNAN/article/details/81807680)

2. [cw2vec理论及其实现](https://www.cnblogs.com/bamtercelboo/p/9044207.html)
