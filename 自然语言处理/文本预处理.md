# 文本预处理

<br>
<br>

## 1. 文本数据的基本特征提取

```python
import pandas as pd
train = pd.read_csv('file_path')
print(train.head(10))
```

```bash
   id  label                                              tweet
0   1      0   @user when a father is dysfunctional and is s...
1   2      0  @user @user thanks for #lyft credit i can't us...
2   3      0                                bihday your majesty
3   4      0  #model   i love u take with u all the time in ...
4   5      0             factsguide: society now    #motivation
5   6      0  [2/2] huge fan fare and big talking before the...
6   7      0   @user camping tomorrow @user @user @user @use...
7   8      0  the next school year is the year for exams.ð��...
8   9      0  we won!!! love the land!!! #allin #cavs #champ...
9  10      0   @user @user welcome here !  i'm   it's so #gr...
```

### 1.1 词汇数量

&emsp;&emsp;对每一条推文，我们可以提取的最基本特征之一就是词语数量。这样做的初衷就是通常情况下，负面情绪评论含有词语数量比正面情绪评论多。

&emsp;&emsp;我们可以简单地调用split函数，将句子切分：

```python
train['word_count']=train['tweet'].apply(lambda x:len(str(x).split(" ")))
train[['tweet','word_count']].head()
```

![](https://upload-images.jianshu.io/upload_images/1531909-12bf948a115c1293.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/421/format/webp)

&emsp;&emsp;注意这里字符串的个数包含了推文中的空格个数，我们根据需要自行去除掉。

### 1.2 字符数量

&emsp;&emsp;选择字符数量作为特征的原因和前一个特征一样。在这里，我们直接通过字符串长度计算每条推文字符数量。

```python
train['char_count']=train['tweet'].str.len()
train[['tweet','char_count']].head()
```

### 1.3 平均词汇长度

&emsp;&emsp;我们接下来将计算每条推文的平均词汇长度作为另一个特征，这个有可能帮助我们改善模型。将每条推文所有单词的长度然后除以每条推文单词的个数，即可作为平均词汇长度。

```python
def avg_word(sentence):
    words=sentence.split()
    return (sum(len(word) for word in words)/len(words))

train['avg_word']=train['tweet'].apply(lambda x:avg_word(x))
train[['tweet','avg_word']].head()
```

![](https://upload-images.jianshu.io/upload_images/1531909-54fa58e1e5c08326.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/410/format/webp)

### 1.4 停用词的数量

&emsp;&emsp;通常情况下，在解决NLP问题时，首要任务时去除停用词（stopword）。但是有时计算停用词的数量可以提供我们之前失去的额外信息。下面关于停用词的解释：

&emsp;&emsp;为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。通常意义上，Stop Words大致为如下两类：

&emsp;&emsp;这些词应用十分广泛，在Internet上随处可见，比如“Web”一词几乎在每个网站上均会出现，对这样的词搜索引擎无 法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率；

&emsp;&emsp;这类就更多了，包括了语气助词、副词、介词、连接词等，通常自身 并无明确的意义，只有将其放入一个完整的句子中才有一定作用，如常见的“的”、“在”之类。

&emsp;&emsp;在这里，我们导入NLTK库中的stopwors模块

```python
from nltk.corpus import stopwords
stop=stopwords.words('english')
train['stopwords']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x in stop]))
train[['tweet','stopwords']].head()
```

![](https://upload-images.jianshu.io/upload_images/1531909-07e2c6d3dcf38855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/383/format/webp)

### 1.5 特殊字符的数量

&emsp;&emsp;一个比较有趣的特征就是我们可以从每个推文中提取“#”和“@”符号的数量。这也有利于我们从文本数据中提取更多信息

&emsp;&emsp;这里我们使用startswith函数来处理。

```python
train['hashtags']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.startswith("#")]))
train[['tweet','hashtags']].head()
```

### 1.6 数字的数量

&emsp;&emsp;这个特征并不常用，但是在做相似任务时，数字数量是一个比较有用的特征。

```python
train['numerics']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.isdigit()]))
train[['tweet','numerics']].head()
```

![](https://upload-images.jianshu.io/upload_images/1531909-9904b8298ec63ac3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/376/format/webp)

### 1.7 大写单词的数量

&emsp;&emsp;“Anger”或者 “Rage”通常情况下使用大写来表述，所以有必要去识别出这些词。

```python
train['upper']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.isupper()]))
train[['tweet','upper']].head()
```

![](https://upload-images.jianshu.io/upload_images/1531909-06a2857e96db9714.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/356/format/webp)

<br>

## 2. 文本数据的基本预处理

### 2.1 去除数据中的非文本部分

```python
import re

# 过滤不了\\ \ 中文（）还有————
r1 = u'[a-zA-Z0-9’!"#$%&\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\]^_`{|}~]+'#用户也可以在此进行自定义过滤字符
# 者中规则也过滤不完全
r2 = "[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+"
# \\\可以过滤掉反向单杠和双杠，/可以过滤掉正向单杠和双杠，第一个中括号里放的是英文符号，第二个中括号里放的是中文符号，第二个中括号前不能少|，否则过滤不完全
r3 =  "[.!//_,$&%^*()<>+\"'?@#-|:~{}]+|[——！\\\\，。=？、：“”‘’《》【】￥……（）]+"
# 去掉括号和括号内的所有内容
r4 =  "\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+""'?@|:~{}#]+|[——！\\\，。=？、：“”‘’￥……（）《》【】]"
# 去掉html
r5 = '<.*?>'

sentence = "<a>hello, wor?d<>"
sentence = re.sub(r5, '',sentence)

# print(sentence)
```

### 2.2 去除指定无用的符号

&emsp;&emsp;我们拿到的文本有时候很有很多空格，或者你不想要的符号，那么你就可以用这个方法去掉所有你不想要的符号。在这里我以空格为例

```python
content = ['  欢迎来到  炼己者的博客','炼己者     带你入门NLP  ']
# 去掉文本中的空格
def process(our_data):
    m1 = map(lambda s: s.replace(' ', ''), our_data)
    return list(m1)
print(process(content))
```

&emsp;&emsp;传入的参数our_data是个列表，此函数可以把文本中的所有空格全部去掉。看一下输出的结果。可以发现，所有的空格都被删掉了。

```python
['欢迎来到炼己者的博客', '炼己者带你入门NLP']
```

&emsp;&emsp;pandas 实现：

```python
train['tweet'] = train['tweet'].str.replace('[^\w\s]','')
train['tweet'].head()
```

```bash
0    user when a father is dysfunctional and is so ...
1    user user thanks for lyft credit i cant use ca...
2                                  bihday your majesty
3    model i love u take with u all the time in urð...
4                    factsguide society now motivation
Name: tweet, dtype: object
```



### 2.3 让文本只保留汉字

&emsp;&emsp;这个操作我最喜欢，他可以去掉所有的符号，包括数字、标点、字母等等

```python
content = ['如果这篇文章对你有所帮助，那就点个赞呗！！！','如果想联系炼己者的话，那就打电话：110！！！','想学习NLP，那就来关注呀！^-^']
# 让文本只保留汉字
def is_chinese(uchar):
    if uchar >= u'\u4e00' and uchar <= u'\u9fa5':
        return True
    else:
        return False

def format_str(content):
    content_str = ''
    for i in content:
        if is_chinese(i):
            content_str = content_str + ｉ
    return content_str

# 参函数传入的是每一句话
chinese_list = []
for line in content:
    chinese_list.append(format_str(line))
print(chinese_list)
```

&emsp;&emsp;然后我们来看一下输出的内容，你会发现只剩下中文了。

```python
['如果这篇文章对你有所帮助那就点个赞呗', '如果想联系炼己者的话那就打电话', '想学习那就来关注呀']
```

<br>

## 2.4 分词

&emsp;&emsp;首先你得下载jieba这个库，直接pip install jieba即可。 我们就以上面处理好的那句话作为例子来操作：

```python
import jieba
sentence = '我们热爱人工智能'
sentence_seg = jieba.cut(sentence)
splited_sentence = ' '.join(sentence_seg)
# print(splited_sentence)
```

<br>

## 2.5 去除停用词

&emsp;&emsp;首先你得上网下载一个停用词表，然后把这份停用词转换为列表 为了方便大家理解，在这里我就假设一个停用词表了,我们以分好词的数据作为输入。

```python
# 分好词的数据
fenci_list = [['如果', '这', '篇文章', '对', '你', '有所', '帮助', '那', '就', '点个', '赞', '呗'],
 ['如果', '想', '联系', '炼己', '者', '的话', '那', '就', '打电话'],
 ['想', '学习', '那', '就', '来', '关注', '呀']]

# 停用词表
stopwords = ['的','呀','这','那','就','的话','如果']

# 去掉文本中的停用词
def drop_stopwords(contents, stopwords):
    contents_clean = []
    for line in contents:
        line_clean = []
        for word in line:
            if word in stopwords:
                continue
            line_clean.append(word)
        contents_clean.append(line_clean)
    return contents_clean

print(drop_stopwords(fenci_list,stopwords))
```

&emsp;&emsp;我们来一下结果，对比发现少了一些停用词。

```python
[['篇文章', '对', '你', '有所', '帮助', '点个', '赞', '呗'],
 ['想', '联系', '炼己', '者', '打电话'],
 ['想', '学习', '来', '关注']]
```

&emsp;&emsp;我觉得上面的操作也可应用在去除一些你不想要的符号上面，你可以把没有用的符号添加到停用词表里，那么它也会被去掉。

&emsp;&emsp;pandas实现：

```python
from nltk.corpus import stopwords
stop=stopwords.words('english')
train['tweet']=train['tweet'].apply(lambda sen:" ".join(x for x in sen.split() if x not in stop))
train['tweet'].head()
```

```bash
0    user father dysfunctional selfish drags kids d...
1    user user thanks lyft credit cant use cause do...
2                                       bihday majesty
3                model love u take u time urð ðððð ððð
4                        factsguide society motivation
Name: tweet, dtype: object
```

### 2.6 常见词去除

&emsp;&emsp;我们可以把常见的单词从文本数据首先,让我们来检查中最常出现的10个字文本数据然后再调用删除或保留。

```python
import pandas as pd
freq=pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]
```

```bash
user     17473
love      2647
ð         2511
day       2199
â         1797
happy     1663
amp       1582
im        1139
u         1136
time      1110
dtype: int64
```

&emsp;&emsp;现在我们把这些词去除掉，因为它们对我们文本数据分类没有任何作用

```python
freq=list(freq.index)
train['tweet']=train['tweet'].apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))
train['tweet'].head()
```

```bash
0    father dysfunctional selfish drags kids dysfun...
1    thanks lyft credit cant use cause dont offer w...
2                                       bihday majesty
3                              model take urð ðððð ððð
4                        factsguide society motivation
Name: tweet, dtype: object
```

### 2.7 稀缺词去除

&emsp;&emsp;同样,正如我们删除最常见的话说,这一次让我们从文本中删除很少出现的词。因为它们很稀有,它们之间的联系和其他词主要是噪音。可以替换罕见的单词更一般的形式,然后这将有更高的计数。

```python
freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]
freq = list(freq.index)
train['tweet'] = train['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
train['tweet'].head()
```

```bash
0    father dysfunctional selfish drags kids dysfun...
1    thanks lyft credit cant use cause dont offer w...
2                                       bihday majesty
3                              model take urð ðððð ððð
4                        factsguide society motivation
Name: tweet, dtype: object
```

## 3. 处理为文本特征

&emsp;&emsp;数据处理到这里，基本上是干净的文本了，现在可以调用sklearn来对我们的文本特征进行处理了。常用的方法如下：

- Bag of Words词袋模型
    - Bow
    - Tf-idf
- N-gram语言模型
    - Bigram
    - Trigram
- Word2vec分布式模型
    - Word2vec

&emsp;&emsp;接下来我将结合代码简单讲解一下Tf-idf，Bigram，word2vec的用法。语言模型这一块内容，可以在之后的文章深入了解。

### 4.1 Tf-idf（Term Frequency-Inverse Document Frequency）

&emsp;&emsp;该模型基于词频，将文本转换成向量，而不考虑词序。假设现在有N篇文档，在其中一篇文档D中，词汇x的TF、IDF、TF-IDF定义如下：

&emsp;&emsp;(1)Term Frequency(TF(x)):指词x在当前文本D中的词频

&emsp;&emsp;(2)Inverse Document Frequency(IDF): N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数，平滑后的IDF如下： IDF(x)=log\frac{N+1}{N(x)+1}+1  

![](https://www.zhihu.com/equation?tex=IDF%28x%29%3Dlog%5Cfrac%7BN%2B1%7D%7BN%28x%29%2B1%7D%2B1+)

&emsp;&emsp;(3)TF-IDF ：

![](https://www.zhihu.com/equation?tex=TF%E2%88%92IDF%28x%29%3DTF%28x%29%E2%88%97IDF%28x%29)

```python
# tf-idf特征：
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
corpus = ["This is sample document.", "another random document.", "third sample document text"]
vector = TfidfVectorizer()
tf_data = vector.fit_transform(corpus)
# print(tf_data)
# print(vector.vocabulary_)
tfidf_df = pd.DataFrame(tf_data.toarray(), columns=vector.get_feature_names())   # # to DataFrame
# print(tfidf_df)
```

&emsp;&emsp;pandas实现：

```python
tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0).reset_index()
import numpy as np
for i,word in enumerate(tf1['words']):
    tf1.loc[i, 'idf'] =np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))
tf1['tfidf']=tf1['tf']*tf1['idf']
```
![](https://upload-images.jianshu.io/upload_images/1531909-ff2c103d2aa858ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/322/format/webp)


### 4.2 N-gram语言模型

&emsp;&emsp;词袋模型不考虑每个单词的顺序。有时候把一句话顺序捣乱，我们可能就看不懂这句话在说什么了，例如：

> 我玩电脑 = 电脑玩我 ？

&emsp;&emsp;N-gram模型是一种语言模型（Language Model），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 Bi-gram (N=2N=2) 和 Tri-gram (N=3N=3)，一般已经够用了。例如,"I love deep learning"，可以分解的 Bi-gram 和 Tri-gram ：

> Bi-gram : {I, love}, {love, deep}, {love, deep}, {deep, learning} 
> Tri-gram : {I, love, deep}, {love, deep, learning}

&emsp;&emsp;sklearn库中的CountVectorizer 有一个参数ngram_range，如果赋值为(2,2)则为Bigram，当然使用语言模型会大大增加我们字典的大小。

```python
# N-gram语言模型
ram_range=(1,1) # 表示 unigram
ngram_range=(2,2)   # 表示 bigram,
ngram_range=(3,3)   # 表示 thirgram
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import jieba

data = ["为了祖国，为了胜利，向我开炮！向我开炮！",
        "记者：你怎么会说出那番话",
        "我只是觉得，对准我自己打"]
data = [' '.join(jieba.lcut(e)) for e in data]  # 分词，并用" "连接
vector = CountVectorizer(min_df=1, ngram_range=(2, 2))  # bigram
X = vector.fit_transform(data)  # 将分词好的文本转换为矩阵
print(vector.vocabulary_)   # 得到特征
print(X)    # (句子下标, 单词特征下标)   频数
ngram_df = pd.DataFrame(X.toarray(), columns = vector.get_feature_names())   # to DataFrame
print(ngram_df.head())
```

### 4.3 Word2vec词向量

&emsp;&emsp;Word2Vec使用一系列的文档的词语去训练模型，把文章的词映射到一个固定长度的连续向量。一般维数较小，通常为100 ~ 500。意义相近的词之间的向量距离较小。它以稠密的向量形式表示单词。有两种模式：

&emsp;&emsp;(1) CBOW（Continuous Bag-Of-Words）：利用词的上下文预测当前的词。

&emsp;&emsp;(2) Skip-Gram：利用当前的词来预测上下文。

&emsp;&emsp;因为word2vector模型的得到的是词向量，如何表示句子呢？最简单的方法就是，将每个句子中的词向量相加取平均值，即每个句子的平均词向量来表示句子的向量。

```python
# word2vec
from gensim.models import Word2Vec
import numpy as np

data = ["I love deep learning","I love studying","I want to travel"]
#词频少于min_count次数的单词会被丢弃掉
#size指特征向量的维度为50
#workers参数控制训练的并行数
train_w2v = Word2Vec(data, min_count=5, size=50, workers=4)
avg_data = []
for row in data:
    vec = np.zeros(50)
    count = 0
    for word in row:
        try:
            vec += train_w2v[word]
            count += 1
        except:
            pass
    avg_data.append(vec/count)
print(avg_data)
```

### 4.4 glove词向量

#### 4.4.1 使用训练好的glove模型

&emsp;&emsp;词嵌入就是文本的向量化表示，潜在思想就是相似单词的向量之间的距离比较短。

```python
from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)
```

#### 4.4.2 训练GloVe中文词向量

[如何训练GloVe中文词向量](http://www.linzehui.me/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/)

[极简使用︱Glove-python词向量训练与使用](https://blog.csdn.net/sinat_26917383/article/details/83029140)



# Reference:

1. [NLP入门-- 文本预处理Pre-processing](https://zhuanlan.zhihu.com/p/53277723)

2. [文本数据处理的终极指南-[NLP入门]](https://www.jianshu.com/p/37e529c8baa9)

3. [如何训练GloVe中文词向量](http://www.linzehui.me/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/)

4. [如何gensim加载glove训练的词向量](https://www.jianshu.com/p/c2a9d3e76706)