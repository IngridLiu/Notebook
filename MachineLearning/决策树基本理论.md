# 决策树基本理论
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，对ID3， C4.5的算法思想做了总结，对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn使用了优化版的CART算法作为其决策树算法的实现。
    </font>
</p>

## 1. 决策树ID3算法的信息论基础
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;机器学习算法其实很古老，作为一个码农经常会不停的敲if,elseif,else,其实就已经在用到决策树的思想了。只是你有没有想过，有这么多条件，用哪个条件特征先做if，哪个条件特征后做if比较优呢？怎么准确的定量选择这个标准就是决策树机器学习算法的关键了。1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。下面我们就看看ID3算法是怎么选择特征的。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;首先，我们需要熟悉信息论中熵的概念。熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下：
    </font>
</p>
<div align="center">
  <img src="https://latex.codecogs.com/gif.latex?H(X)&space;=&space;-\sum\limits_{i=1}^{n}p_i&space;logp_i" title="H(X) = -\sum\limits_{i=1}^{n}p_i logp_i" />
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;其中n代表X的n种不同的离散取值。而<img src="https://latex.codecogs.com/gif.latex?\inline&space;p_i" title="p_i" />代表了X取值为i的概率，log为以2或者e为底的对数。举个例子，比如X有2个可能的取值，而这两个取值各为1/2时X的熵最大，此时X具有最大的不确定性。值为<img src="https://latex.codecogs.com/gif.latex?\inline&space;(&space;H(X)&space;=&space;-(\frac{1}{2}log\frac{1}{2}&space;&plus;&space;\frac{1}{2}log\frac{1}{2})&space;=&space;log2" title="( H(X) = -(\frac{1}{2}log\frac{1}{2} + \frac{1}{2}log\frac{1}{2}) = log2" />。如果一个值概率大于1/2，另一个值概率小于1/2，则不确定性减少，对应的熵也会减少。比如一个概率1/3，一个概率2/3，则对应熵为<img src="https://latex.codecogs.com/gif.latex?\inline&space;H(X)&space;=&space;-(\frac{1}{3}log\frac{1}{3}&space;&plus;&space;\frac{2}{3}log\frac{2}{3})&space;=&space;log3&space;-&space;\frac{2}{3}log2&space;<&space;log2)" title="H(X) = -(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3}) = log3 - \frac{2}{3}log2 < log2)" />。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;熟悉了一个变量X的熵，很容易推广到多个个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：
    </font>
</p>
<div align="center">
    <img src="https://latex.codecogs.com/gif.latex?H(X,Y)&space;=&space;-\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)" title="H(X,Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)" />
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;有了联合熵，又可以得到条件熵的表达式H(X|Y)，条件熵类似于条件概率,它度量了我们的X在知道Y以后剩下的不确定性。表达式如下：
    </font>
</p>
<div align="center">
    <img src="https://latex.codecogs.com/gif.latex?H(X|Y)&space;=&space;-\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i)&space;=&space;\sum\limits_{j=1}^{n}p(y_j)H(X|y_j)" title="H(X|Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)" />
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;好吧，绕了一大圈，终于可以重新回到ID3算法了。我们刚才提到H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？从上面的描述大家可以看出，它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;上面一堆概念，大家估计比较晕，用下面这个图很容易明白他们的关系。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。
    </font>
</p>
<div align="center">
    <a href="https://imgbb.com/"><img src="https://image.ibb.co/fAFO9S/1042406_20161110123427608_582642065.png" alt="1042406_20161110123427608_582642065" border="0"></a>
</div>

## 2 决策树ID3算法
### 2.1 决策树ID3算法的思路
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;上面提到ID3算法就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。这里我们举一个信息增益计算的具体的例子。比如我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0.
    </font>
</p>
<p>
    <font face="宋体" size=5>    
        &emsp;&emsp;样本D的熵为：<img src="https://latex.codecogs.com/gif.latex?\inline&space;H(D)&space;=&space;-(\frac{9}{15}log_2\frac{9}{15}&space;&plus;&space;\frac{6}{15}log_2\frac{6}{15})&space;=&space;0.971" title="H(D) = -(\frac{9}{15}log_2\frac{9}{15} + \frac{6}{15}log_2\frac{6}{15}) = 0.971" />
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;样本D在特征下的条件熵为：
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;<img src="https://latex.codecogs.com/gif.latex?H(D|A)&space;=&space;\frac{5}{15}H(D1)&space;&plus;&space;\frac{5}{15}H(D2)&space;&plus;&space;\frac{5}{15}H(D3)=&space;-\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5}&space;&plus;&space;\frac{2}{5}log_2\frac{2}{5})&space;-&space;\frac{5}{15}(\frac{2}{5}log_2\frac{2}{5}&space;&plus;&space;\frac{3}{5}log_2\frac{3}{5})&space;-\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5}&space;&plus;&space;\frac{1}{5}log_2\frac{1}{5})&space;=&space;0.888" title="H(D|A) = \frac{5}{15}H(D1) + \frac{5}{15}H(D2) + \frac{5}{15}H(D3)= -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) -\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) = 0.888" />
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;对应的信息增益为：<img src="https://latex.codecogs.com/gif.latex?\inline&space;I(D,A)&space;=&space;H(D)&space;-&space;H(D|A)&space;=&space;0.083" title="I(D,A) = H(D) - H(D|A) = 0.083" />
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;下面我们看看具体算法过程大概是怎么样的。
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;输入为训练数据集D，特征集A，输出为决策树T。
    </font>
</p>        
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;算法的过程为：
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）初始化信息增益的阈值ϵ
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）若D中所有实例均属于同一类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />，则T为单节点树，并将类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />。作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;3）若A=<img src="https://latex.codecogs.com/gif.latex?\inline&space;\varnothing" title="\varnothing" />，则T为单节点树，并将D中实例数最大的类作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;4）否则，计算A中各个特征对D的信息增益，选择信息增益最大的特征<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;5）如果<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的信息增益小于阈值ϵ，则置T为单节点树，并将D中实例数最大的类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;6）否则，对<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的每一可能值<img src="https://latex.codecogs.com/gif.latex?\inline&space;a_i" title="a_i" />，依<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g=a_i" title="A_g=a_i" />将D分割为若干非空子集<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />，将<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />中实例数最大的类作为标记，构建子节点，由节点及其子结构点构成树T，返回<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;7）对第i个子节点，以<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />为训练集，以<img src="https://latex.codecogs.com/gif.latex?\inline&space;A-{A_g}" title="A-{A_g}" />为特征集，递归地调用2）～6）步，得到子树<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />，返回<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />。
    </font>
</p>


### 2.2 决策树ID3算法的不足
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）ID3没有考虑连续特征，比如长度、密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）ID3采用信息增益大的特征优先建立决策树的节点，很快就被人发现，在同等条件下，取值比较多的特征比取值少的特征信息增益大。比如，一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实它们都是完全不确定的变量，但是取三个值的比取2个值的信息增益大。如果校正这个问题呢？
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;3）ID3对于缺失值的情况没有做考虑。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;4）ID3没有考虑过拟合的问题
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;ID3算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。也许你会问，为什么不叫ID4，ID5之类的名字呢?那是因为决策树太火爆，他的ID3一出来，别人二次创新，很快 就占了ID4， ID5，所以他另辟蹊径，取名C4.0算法，后来的进化版为C4.5算法。下面我们就来聊下C4.5算法。
    </font>
</p>

## 3 决策树C4.5算法
## 3.1 决策树C4.5算法的改进
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;上一节讲到ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，第三个是缺失值处理的问题，第四个是过拟合的问题。昆兰在C4.5算法中改进了上述4个问题。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第一个问题，不能处理连续特征，C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个值，从小到大取值为<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_1,a_2,...,a_m}" title="{a_1,a_2,...,a_m}" />，则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点<img src="https://latex.codecogs.com/gif.latex?\inline&space;{T_i}" title="{T_i}" />表示为<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i&space;=&space;\frac{a_i&plus;a_{i&plus;1}}{2}" title="T_i = \frac{a_i+a_{i+1}}{2}" />。对于这m-1个点，分别计算以该点作为二分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />，则小于<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />的值为类别1，大于<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。引入一个信息增益比的变量<img src="https://latex.codecogs.com/gif.latex?\inline&space;{I_R(X,Y)}" title="{I_R(X,Y)}" />，它是信息增益和特征熵的比值。表达式如下：
    </font>
</p>
<div align="center">
    ？？？<img src="https://latex.codecogs.com/gif.latex?I_R(C,A)&space;=&space;\frac{I(A,C)}{H_A(C)}" title="I_R(C,A) = \frac{I(A,C)}{H_A(C)}" />
</div>
<p>
    <font face="宋体" size=5>   
        ？？？&emsp;&emsp;其中C为样本的输出类别，A为样本特征，对于特征熵<img src="https://latex.codecogs.com/gif.latex?\inline&space;H_A(C)" title="H_A(C)" />，表达式如下：
    </font>
</p>
<div align="center">
    ？？？<img src="https://latex.codecogs.com/gif.latex?H_A(C)&space;=&space;-\sum\limits_{i=1}^{n}\frac{|C_i|}{|C|}log_2\frac{|C_i|}{|C|}" title="H_A(C) = -\sum\limits_{i=1}^{n}\frac{|C_i|}{|C|}log_2\frac{|C_i|}{|C|}" />
</div>
<p>
    <font face="宋体" size=5>   
        ？？？&emsp;&emsp;其中n为特征A的类别数，<img src="https://latex.codecogs.com/gif.latex?\inline&space;|C_i|" title="|C_i|" />为特征A的第i个取值对应的样本数。|C|为样本个数。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;特征数越多的特征对应的特征熵越大，特征熵作为分母，可以校正信息增益容易偏向取值较多的特征的问题。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征值缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征值的样本的处理。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征A有特征值的数据D1，另一部分是特征A没有特征值的数据D2，然后将特征A没有缺失特征值的数据集D1和A特征对应的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是特征A有特征值的样本加权后所占加权总样本的比例。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第二个子问题，可以将缺失特征值的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有三个特征值A1，A2，A3。3个特征值对应的A特征有特征值的样本个数为2、3、4，则a同时划分入A1，A2，A3。对应权重调节为2/9，3/9，4/9。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第4个问题，C4.5引入了正则化系数进行初步的剪枝。具体方法这里不讨论。下篇讲CART的时候会详细讨论剪枝的思路。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;除了上面的4点，C4.5和ID的思路区别不大
    </font>
</p>


###3.2 决策树C4.5算法的不足与思考
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）由于决策树算法非常容易过拟合，因此对于生成的决策树必须进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。后面在讲CART树时会专门讲解决策树的剪枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;
    </font>
</p>
