# 决策树基本理论
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，上篇对ID3， C4.5的算法思想做了总结，下篇重点对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn使用了优化版的CART算法作为其决策树算法的实现。
    </font>
</p>

## 1. 决策树ID3算法的信息论基础
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;机器学习算法其实很古老，作为一个码农经常会不停的敲if,elseif,else,其实就已经在用到决策树的思想了。只是你有没有想过，有这么多条件，用哪个条件特征先做if，哪个条件特征后做if比较优呢？怎么准确的定量选择这个标准就是决策树机器学习算法的关键了。1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。下面我们就看看ID3算法是怎么选择特征的。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;首先，我们需要熟悉信息论中熵的概念。熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下：
    </font>
</p>
<div align="center">
  <img src="https://latex.codecogs.com/gif.latex?H(X)&space;=&space;-\sum\limits_{i=1}^{n}p_i&space;logp_i" title="H(X) = -\sum\limits_{i=1}^{n}p_i logp_i" />
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;其中n代表X的n种不同的离散取值。而<img src="https://latex.codecogs.com/gif.latex?\inline&space;p_i" title="p_i" />代表了X取值为i的概率，log为以2或者e为底的对数。举个例子，比如X有2个可能的取值，而这两个取值各为1/2时X的熵最大，此时X具有最大的不确定性。值为\\( H(X) = -(\frac{1}{2}log\frac{1}{2} + \frac{1}{2}log\frac{1}{2}) = log2 \\)。如果一个值概率大于1/2，另一个值概率小于1/2，则不确定性减少，对应的熵也会减少。比如一个概率1/3，一个概率2/3，则对应熵为H(X) = -(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3}) = log3 - \frac{2}{3}log2 < log2)。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;熟悉了一个变量X的熵，很容易推广到多个个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：
    </font>
</p>
<div align="center">
    <img src="https://latex.codecogs.com/gif.latex?H(X,Y)&space;=&space;-\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)" title="H(X,Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)" />
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;有了联合熵，又可以得到条件熵的表达式H(X|Y)，条件熵类似于条件概率,它度量了我们的X在知道Y以后剩下的不确定性。表达式如下：
    </font>
</p>
<p>
    <img src="https://latex.codecogs.com/gif.latex?H(X|Y)&space;=&space;-\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i)&space;=&space;\sum\limits_{j=1}^{n}p(y_j)H(X|y_j)" title="H(X|Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)" />
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;好吧，绕了一大圈，终于可以重新回到ID3算法了。我们刚才提到H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？从上面的描述大家可以看出，它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;上面一堆概念，大家估计比较晕，用下面这个图很容易明白他们的关系。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。
    </font>
</p>
<div>
    ![](https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161110123427608-582642065.png)
</div>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;
    </font>
</p>