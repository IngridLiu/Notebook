# 决策树基本理论

<p>
    <font face="宋体" size=5>
        &emsp;&emsp;决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法。同时也特别适合集成学习比如随机森林。
    </font>
</p>
<font>
    <p>&emsp;&emsp;决策树学习，假定给定训练数据集：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?D={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}" title="D={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}" />
        </div>
    </p>
    <p>&emsp;&emsp;其中，<img src="http://latex.codecogs.com/gif.latex?\inline&space;x_i=(x_i^{(1)},&space;x_i^{(1)},...,x_i^{(n)})^T" title="x_i=(x_i^{(1)}, x_i^{(1)},...,x_i^{(n)})^T" />为输入实例（特征向量），n为特征个数，<img src="http://latex.codecogs.com/gif.latex?\inline&space;y_i\in&space;\{{1,2,...,K}\}" title="y_i\in \{{1,2,...,K}\}" />为类标记，<img src="http://latex.codecogs.com/gif.latex?\inline&space;i=1,2,...,N" title="i=1,2,...,N" />，N为样本容量。学习目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行准确的分类。</p>
    <p>&emsp;&emsp;决策树学习算法包含特征选择、决策树生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对的，决策树的剪枝则考虑全局最优。</p>
</font>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;本文就对决策树算法原理做一个总结，对ID3， C4.5的算法思想做了总结，对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn使用了优化版的CART算法作为其决策树算法的实现。
    </font>
</p>


<br>

## 1. 决策树ID3算法的信息论基础
### 1.1 熵的定义
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;机器学习算法其实很古老，作为一个码农经常会不停的敲if,elseif,else,其实就已经在用到决策树的思想了。只是你有没有想过，有这么多条件，用哪个条件特征先做if，哪个条件特征后做if比较优呢？怎么准确的定量选择这个标准就是决策树机器学习算法的关键了。1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。下面我们就看看ID3算法是怎么选择特征的。
    </font>
</p>
<font face="宋体" size=5>
    <p>&emsp;&emsp;首先，我们需要熟悉信息论中熵的概念。熵（entropy）度量了事物的不确定性，越不确定的事物，它的熵就越大。</p>
    <p>&emsp;&emsp;设X是一个取有限个值的离散随机变量，其概率分布为：</p>
    <p>
        <div align="center"><img src="http://latex.codecogs.com/gif.latex?P(X=x_i)=p_i,&space;i=1,2,...,n" title="P(X=x_i)=p_i, i=1,2,...,n" /></div>
    </p>  
    <p>&emsp;&emsp;则随机变量X的熵定义为：</p> 
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?H(X)&space;=&space;-\sum\limits_{i=1}^{n}p_i&space;logp_i" title="H(X) = -\sum\limits_{i=1}^{n}p_i logp_i}" />
        </div>
    </p> 
    <p>&emsp;&emsp;其中n代表X的n种不同的离散取值。而<img src="https://latex.codecogs.com/gif.latex?\inline&space;p_i" title="p_i" />代表了X取值为i的概率。log为以2或者e为底（自然对数）的对数，此时熵的单位分别称为比特（bit）或纳特（nat）。</p>   
    <p>&emsp;&emsp;由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可以将X的熵记为<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(p)" title="H(p)" />，即：</p>
    <p>
        <div align="center">
            &emsp;&emsp;<img src="http://latex.codecogs.com/gif.latex?H(p)&space;=&space;-\sum\limits_{i=1}^{n}p_i&space;logp_i" title="H(p) = -\sum\limits_{i=1}^{n}p_i logp_i" />
        </div>
    </p>
    <p>&emsp;&emsp;熵越大，随机变量的不确定性就越大，从定义可以验证：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?0\leqslant&space;H(p)\leqslant&space;logn" title="0\leqslant H(p)\leqslant logn" />
        </div>
    </p>
    <p>
        &emsp;&emsp;举个例子，比如X有2个可能的取值，而这两个取值各为1/2时X的熵最大，此时X具有最大的不确定性。值为<img src="https://latex.codecogs.com/gif.latex?\inline&space;(&space;H(X)&space;=&space;-(\frac{1}{2}log\frac{1}{2}&space;&plus;&space;\frac{1}{2}log\frac{1}{2})&space;=&space;log2" title="( H(X) = -(\frac{1}{2}log\frac{1}{2} + \frac{1}{2}log\frac{1}{2}) = log2" />。如果一个值概率大于1/2，另一个值概率小于1/2，则不确定性减少，对应的熵也会减少。比如一个概率1/3，一个概率2/3，则对应熵为<img src="https://latex.codecogs.com/gif.latex?\inline&space;H(X)&space;=&space;-(\frac{1}{3}log\frac{1}{3}&space;&plus;&space;\frac{2}{3}log\frac{2}{3})&space;=&space;log3&space;-&space;\frac{2}{3}log2&space;<&space;log2)" title="H(X) = -(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3}) = log3 - \frac{2}{3}log2 < log2)" />。
    </p> 
</font>

<br>
<font face="宋体" size=5>
    <p>&emsp;&emsp;设有随机变量<img src="http://latex.codecogs.com/gif.latex?\inline&space;(X,Y)" title="(X,Y)" />,其联合概率分布为:</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?P(X=x_i,Y=y_j)=p_ij,&space;\quad&space;i=1,2,...,n;\quad&space;j=1,2,...,m" title="P(X=x_i,Y=y_j)=p_ij, \quad i=1,2,...,n;\quad j=1,2,...,m" />
        </div>
    </p>
    <p>&emsp;&emsp;条件熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(Y|X)" title="H(Y|X)" />表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵（conditional entropy）<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(Y|X)" title="H(Y|X)" />。将条件熵定义为X给定条件下Y的条件概率分布的熵对X的数学期望：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?H(Y|X)&space;=&space;-\sum\limits_{i=1}^{n}p_i&space;H(Y|X=x_i)" title="H(Y|X) = -\sum\limits_{i=1}^{n}p_i H(Y|X=x_i)" />
        </div>
    </p>
    <p>&emsp;&emsp;这里，<img src="http://latex.codecogs.com/gif.latex?\inline&space;p_i=P(X=x_i),\quad&space;i=1,2,...,n" title="p_i=P(X=x_i),\quad i=1,2,...,n" />。</p>
    <p>&emsp;&emsp;当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为（empirical entropy）和经验条件熵（empirical conditional entropy）。此时，如果有0概率，令0log0=0。</p>
    <p></p>
    
    
</font>


### 1.2 信息增益的定义
<font face="宋体" size=5>
    <p>&emsp;&emsp;信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。将特征A对训练数据集D的信息增益<img src="http://latex.codecogs.com/gif.latex?\inline&space;g(D,A)" title="g(D,A)" />,定义为集合D的经验熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(D)" title="H(D)" />与特征A给定条件下D的经验条件熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(D|A)" title="H(D|A)" />之差，即：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?g(D,A)=H(D)-H(D|A)" title="g(D,A)=H(D)-H(D|A)" />
        </div>
    </p>
    <p>&emsp;&emsp;一般地，熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(Y)" title="H(Y)" />与条件熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(Y|X)" title="H(Y|X)" />之差称为互信息（mutual information）。决策树中的信息增益等价于训练数据集中类与特征的互信息。</p>
    <p>&emsp;&emsp;根据信息增益准则的特征选择方法是：对每个训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>    
</font>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;上面一堆概念，大家估计比较晕，用下面这个图很容易明白他们的关系。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。
    </font>
</p>
<div align="center">
    <a href="https://imgbb.com/"><img src="https://image.ibb.co/fAFO9S/1042406_20161110123427608_582642065.png" alt="1042406_20161110123427608_582642065" border="0"></a>
</div>


### 1.2.1 信息增益的算法
<font face="宋体" size=5>
    <p>&emsp;&emsp;设训练数据集为<img src="http://latex.codecogs.com/gif.latex?\inline&space;D" title="D" />，<img src="http://latex.codecogs.com/gif.latex?\inline&space;|D|" title="|D|" />表示其样本容量，即样本个数。</p>
    <p>&emsp;&emsp;设有<img src="http://latex.codecogs.com/gif.latex?\inline&space;K" title="K" />个类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />，<img src="http://latex.codecogs.com/gif.latex?\inline&space;k=1,2,...,K" title="k=1,2,...,K" />,<img src="http://latex.codecogs.com/gif.latex?\inline&space;|C_k|" title="|C_k|" />为属于类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />的样本个数，<img src="http://latex.codecogs.com/gif.latex?\inline&space;\sum_{k=1}^K|C_k|=|D|" title="\sum_{k=1}^K|C_k|=|D|" />。</p>
    <p>&emsp;&emsp;设特征A有n个不同的取值<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{a_1,a_2,...,a_n\}" title="\{a_1,a_2,...,a_n\}" />,根据特征A的取值将D划分为n个子集<img src="http://latex.codecogs.com/gif.latex?\inline&space;{D_1,D_2,...,D_n}" title="{D_1,D_2,...,D_n}" />，<img src="http://latex.codecogs.com/gif.latex?\inline&space;|D_i|" title="|D_i|" />为<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />的样本个数，<img src="http://latex.codecogs.com/gif.latex?\inline&space;\sum_{i=i}^n|D_i|=|D|" title="\sum_{i=i}^n|D_i|=|D|" />。记子集<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />中属于类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />的样本的集合为<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_ik" title="D_ik" />，即<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_ik=D_i\cap&space;C_k" title="D_ik=D_i\cap C_k" />,|D_ik|为<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_ik" title="D_ik" />的样本个数。</p>
    <p>&emsp;&emsp;于是，信息增益的算法如下：</p>
    <p>&emsp;&emsp;输入：训练数据集D和特征A；</p>
    <p>&emsp;&emsp;特征A对训练数据集D的信息增益。</p>
    <p>&emsp;&emsp;1）计算数据集D的经验熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(D)" title="H(D)" /></p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?H(D)=-\sum_{k=1}^K&space;\frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}" title="H(D)=-\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}" />     
        </div>
    </p>
    <p>&emsp;&emsp;2）计算特征A对数据集D的经验条件熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H(D|A)" title="H(D|A)" /></p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?H(D|A)=-\sum_{i=1}^n\frac{D_i}{D}H(D_i)=\sum_{i=1}^n\frac{D_i}{D}\sum_{k=1}^K\frac{D_{ik}}{D_i}log_2\frac{D_{ik}}{D_i}" title="H(D|A)=-\sum_{i=1}^n\frac{D_i}{D}H(D_i)=\sum_{i=1}^n\frac{D_i}{D}\sum_{k=1}^K\frac{D_{ik}}{D_i}log_2\frac{D_{ik}}{D_i}" />
        </div>
    </p>
    <p>&emsp;&emsp;3）计算信息增益</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?g(D,A)=H(D)-H(D|A)" title="g(D,A)=H(D)-H(D|A)" />
        </div>
    </p>   
</font>


### 1.3 信息增益比的定义
<font face="宋体" size=5>
    <p>&emsp;&emsp;以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。</p>
    <p>&emsp;&emsp;特征A对训练数据集D的信息增益比<img src="http://latex.codecogs.com/gif.latex?\inline&space;g_R(D,A)" title="g_R(D,A)" />定义为其信息增益<img src="http://latex.codecogs.com/gif.latex?\inline&space;g(D,A)" title="g(D,A)" />与训练数据集D关于特征A的值的熵<img src="http://latex.codecogs.com/gif.latex?\inline&space;H_A(D)" title="H_A(D)" /> 之比，即：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?g_R(D,A)=\frac{g(D,A)}{H_A(D)}" title="g_R(D,A)=\frac{g(D,A)}{H_A(D)}" />
        </div>
    </p>
    <p>&emsp;&emsp;其中，<img src="http://latex.codecogs.com/gif.latex?\inline&space;H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}" title="H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}" />,n是特征A取值的个数。</p>
    <p></p>
    <p></p>
    
    
    
</font>


<br>

## 2 决策树ID3算法
### 2.1 决策树ID3算法的思路
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;上面提到ID3算法就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。这里我们举一个信息增益计算的具体的例子。比如我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0.
    </font>
</p>
<p>
    <font face="宋体" size=5>    
        &emsp;&emsp;样本D的熵为：<img src="https://latex.codecogs.com/gif.latex?\inline&space;H(D)&space;=&space;-(\frac{9}{15}log_2\frac{9}{15}&space;&plus;&space;\frac{6}{15}log_2\frac{6}{15})&space;=&space;0.971" title="H(D) = -(\frac{9}{15}log_2\frac{9}{15} + \frac{6}{15}log_2\frac{6}{15}) = 0.971" />
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;样本D在特征下的条件熵为：
    </font>
</p>
<p>
    <font face="宋体" size=5>
        &emsp;&emsp;<img src="https://latex.codecogs.com/gif.latex?H(D|A)&space;=&space;\frac{5}{15}H(D1)&space;&plus;&space;\frac{5}{15}H(D2)&space;&plus;&space;\frac{5}{15}H(D3)=&space;-\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5}&space;&plus;&space;\frac{2}{5}log_2\frac{2}{5})&space;-&space;\frac{5}{15}(\frac{2}{5}log_2\frac{2}{5}&space;&plus;&space;\frac{3}{5}log_2\frac{3}{5})&space;-\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5}&space;&plus;&space;\frac{1}{5}log_2\frac{1}{5})&space;=&space;0.888" title="H(D|A) = \frac{5}{15}H(D1) + \frac{5}{15}H(D2) + \frac{5}{15}H(D3)= -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) -\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) = 0.888" />
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;对应的信息增益为：<img src="https://latex.codecogs.com/gif.latex?\inline&space;I(D,A)&space;=&space;H(D)&space;-&space;H(D|A)&space;=&space;0.083" title="I(D,A) = H(D) - H(D|A) = 0.083" />
    </font>
</p>


### 2.2 决策树ID3算法步骤
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;下面我们看看具体算法过程大概是怎么样的。
    </font>
</p>
<p>
    <font face="宋体" size=5>        
        &emsp;&emsp;输入为训练数据集D，特征集A，输出为决策树T。
    </font>
</p>        
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;算法的过程为：
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）初始化信息增益的阈值ϵ
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）若D中所有实例均属于同一类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />，则T为单节点树，并将类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />。作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;3）若A=<img src="https://latex.codecogs.com/gif.latex?\inline&space;\varnothing" title="\varnothing" />，则T为单节点树，并将D中实例数最大的类作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;4）否则，计算A中各个特征对D的信息增益，选择信息增益最大的特征<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;5）如果<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的信息增益小于阈值ϵ，则置T为单节点树，并将D中实例数最大的类<img src="https://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />作为该节点的类标记，返回T；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;6）否则，对<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的每一可能值<img src="https://latex.codecogs.com/gif.latex?\inline&space;a_i" title="a_i" />，依<img src="https://latex.codecogs.com/gif.latex?\inline&space;A_g=a_i" title="A_g=a_i" />将D分割为若干非空子集<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />，将<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />中实例数最大的类作为标记，构建子节点，由节点及其子结构点构成树T，返回<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />；
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;7）对第i个子节点，以<img src="https://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />为训练集，以<img src="https://latex.codecogs.com/gif.latex?\inline&space;A-{A_g}" title="A-{A_g}" />为特征集，递归地调用2）～6）步，得到子树<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />，返回<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />。
    </font>
</p>


### 2.3 决策树ID3算法的不足
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）ID3没有考虑连续特征，比如长度、密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）ID3采用信息增益大的特征优先建立决策树的节点，很快就被人发现，在同等条件下，取值比较多的特征比取值少的特征信息增益大。比如，一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实它们都是完全不确定的变量，但是取三个值的比取2个值的信息增益大。如果校正这个问题呢？
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;3）ID3对于缺失值的情况没有做考虑。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;4）ID3没有考虑过拟合的问题
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;ID3算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。也许你会问，为什么不叫ID4，ID5之类的名字呢?那是因为决策树太火爆，他的ID3一出来，别人二次创新，很快 就占了ID4， ID5，所以他另辟蹊径，取名C4.0算法，后来的进化版为C4.5算法。下面我们就来聊下C4.5算法。
    </font>
</p>


<br>

## 3 决策树C4.5算法
### 3.1 决策树C4.5算法的改进
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;上一节讲到ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，第三个是缺失值处理的问题，第四个是过拟合的问题。昆兰在C4.5算法中改进了上述4个问题。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第一个问题，不能处理连续特征，C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个值，从小到大取值为<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_1,a_2,...,a_m}" title="{a_1,a_2,...,a_m}" />，则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点<img src="https://latex.codecogs.com/gif.latex?\inline&space;{T_i}" title="{T_i}" />表示为<img src="https://latex.codecogs.com/gif.latex?\inline&space;T_i&space;=&space;\frac{a_i&plus;a_{i&plus;1}}{2}" title="T_i = \frac{a_i+a_{i+1}}{2}" />。对于这m-1个点，分别计算以该点作为二分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />，则小于<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />的值为类别1，大于<img src="https://latex.codecogs.com/gif.latex?\inline&space;{a_t}" title="{a_t}" />的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。引入一个信息增益比的变量<img src="https://latex.codecogs.com/gif.latex?\inline&space;{g_R(D,A)}" title="{g_R(D,A)}" />，它是信息增益和特征熵的比值。表达式如下：
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征值缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征值的样本的处理。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征A有特征值的数据D1，另一部分是特征A没有特征值的数据D2，然后将特征A没有缺失特征值的数据集D1和A特征对应的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是特征A有特征值的样本加权后所占加权总样本的比例。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第二个子问题，可以将缺失特征值的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有三个特征值A1，A2，A3。3个特征值对应的A特征有特征值的样本个数为2、3、4，则a同时划分入A1，A2，A3。对应权重调节为2/9，3/9，4/9。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;对于第4个问题，C4.5引入了正则化系数进行初步的剪枝。具体方法这里不讨论。下篇讲CART的时候会详细讨论剪枝的思路。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;除了上面的4点，C4.5和ID的思路区别不大。
    </font>
</p>


### 3.2 决策树C4.5的具体算法
<font face="宋体" size=5>
    <p>&emsp;&emsp;C4.5的生成算法：</p>
    <p>&emsp;&emsp;输入:训练数据集D，特征集A，阈值<img src="http://latex.codecogs.com/gif.latex?\inline&space;\varepsilon" title="\varepsilon" />；</p>
    <p>&emsp;&emsp;决策树T。</p>
    <p>&emsp;&emsp;1）如果D中所有实例属于同一类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />，则置T为单节点树，并将<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />作为该节点的类，返回T；</p>
    <p>&emsp;&emsp;2）如果<img src="http://latex.codecogs.com/gif.latex?\inline&space;A=\o" title="A=\o" />，则置T为单节点树，并将D中实例数最大的类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />作为该节点的类，返回T；</p>
    <p>&emsp;&emsp;3）否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征<img src="http://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />；</p>
    <p>&emsp;&emsp;4）如果<img src="http://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的信息增益比小于阈值<img src="http://latex.codecogs.com/gif.latex?\inline&space;\varepsilon" title="\varepsilon" />，则置T为单节点树，并将D中实例数最大的类<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />作为该节点的类，返回T；</p>
    <p>&emsp;&emsp;5）否则，对<img src="http://latex.codecogs.com/gif.latex?\inline&space;A_g" title="A_g" />的每一可能值<img src="http://latex.codecogs.com/gif.latex?\inline&space;a_i" title="a_i" />，依<img src="http://latex.codecogs.com/gif.latex?\inline&space;A_g=a_i" title="A_g=a_i" />将D分割为若干非空子集<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />，将<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T。</p>
    <p>&emsp;&emsp;6）对结点i，以<img src="http://latex.codecogs.com/gif.latex?\inline&space;D_i" title="D_i" />为训练集，以<img src="http://latex.codecogs.com/gif.latex?\inline&space;A-\{A_g\}" title="A-\{A_g\}" />为特征集，递归调用1）～5）步，得到子树<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />,返回<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />。</p>
</font>


### 3.3 决策树C4.5算法的不足与思考
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;1）由于决策树算法非常容易过拟合，因此对于生成的决策树必须进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。后面在讲CART树时会专门讲解决策树的剪枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;2）C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;3）C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;4）C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;这4个问题在CART树里面部分加以了改进。所以目前如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。scikit-learn的决策树使用的也是CART算法。
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;
    </font>
</p>
<p>
    <font face="宋体" size=5>   
        &emsp;&emsp;
    </font>
</p>

## 4 CART算法
<font face="宋体" size=5>
    <p>&emsp;&emsp;第二、三节讲到了决策树里ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。CART算法也就是我们下面的重点了。由于CART算法可以做回归，也可以做分类，我们分别加以介绍，先从CART分类树算法开始，重点比较和C4.5算法的不同点。接着介绍CART回归树算法，重点介绍和CART分类树的不同点。然后我们讨论CART树的建树算法和剪枝算法，最后总结决策树算法的优缺点。</p>
</font>

### 4.1 CART分类算法的最有特征选择方法
<font face="宋体" size=5>
    <p>&emsp;&emsp;我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</p>
    <p>&emsp;&emsp;具体的，在分类问题中，假设有K个类别，第k个类别的概率为<img src="http://latex.codecogs.com/gif.latex?\inline&space;p_k" title="p_k" />,则基尼系数的表达式为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?Gini(p)&space;=&space;\sum\limits_{k=1}^{K}p_k(1-p_k)&space;=&space;1-&space;\sum\limits_{k=1}^{K}p_k^2" title="Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2" />
        </div>
    </p>
    <p>&emsp;&emsp;如果是二分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?Gini(p)&space;=&space;2p(1-p)" title="Gini(p) = 2p(1-p)" />
        </div>
    </p>
    <p>&emsp;&emsp;对于个给定的训练数据集D,假设有K个类别, 第k个类别的数量为<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_k" title="C_k" />，则训练数据集的基尼系数表达式为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?\inline&space;Gini(D)&space;=&space;1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2" title="Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2" />
        </div>
    </p>
    <p>&emsp;&emsp;特别的，对于训练数据集D，如果根据特征A的某个值a，把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?\inline&space;Gini(D,A)&space;=&space;\frac{|D_1|}{|D|}Gini(D_1)&space;&plus;&space;\frac{|D_2|}{|D|}Gini(D_2)" title="Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)" />
        </div>
    </p>
    <p>&emsp;&emsp;大家可以比较下基尼系数表达式和熵模型的表达式，二次运算是不是比对数简单很多？尤其是二类分类的计算，更加简单。但是简单归简单，和熵模型的度量方式比，基尼系数对应的误差有多大呢？对于二类分类，基尼系数和熵之半的曲线如下：</p>
    <p>
        <div align="center">
            <a href="https://imgbb.com/"><img src="https://image.ibb.co/kb0BFd/1042406_20161111105202170_1563882835.jpg" alt="1042406_20161111105202170_1563882835" border="0"></a>
        </div>
    </p>
    <p>&emsp;&emsp;从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。</p>    
</font>


### 4.2 CART分类树算法对于连续特征和离散特征处理的改进
<font face="宋体" size=5>
    <p>&emsp;&emsp;对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。</p>
    <p>&emsp;&emsp;体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为<img src="http://latex.codecogs.com/gif.latex?\inline&space;{a_1,a_2,...,a_m}" title="{a_1,a_2,...,a_m}" />,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点，其中第i个划分点<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_i" title="T_i" />表示为：<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_i&space;=&space;\frac{a_i&plus;a_{i&plus;1}}{2}" title="T_i = \frac{a_i+a_{i+1}}{2}" />。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为<img src="http://latex.codecogs.com/gif.latex?\inline&space;a_t" title="a_t" />，则小于<img src="http://latex.codecogs.com/gif.latex?\inline&space;a_t" title="a_t" />的值为类别1，大于<img src="http://latex.codecogs.com/gif.latex?\inline&space;a_t" title="a_t" />的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
    <p>&emsp;&emsp;对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。</p>
    <p>&emsp;&emsp;回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A1\}" title="\{A1\}" />和<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A2,A3\}" title="\{A2,A3\}" />、<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A2\}" title="\{A2\}" />和<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A1,A3\}" title="\{A1,A3\}" />、<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A3\}" title="\{A3\}" />和<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A1,A2\}" title="\{A1,A2\}" />三种情况，找到基尼系数最小的组合，比如<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A2\}" title="\{A2\}" />和<img src="http://latex.codecogs.com/gif.latex?\inline&space;\{A1,A3\}" title="\{A1,A3\}" />，然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。</p>
</font>


### 4.3 CART分类树建立算法的具体流程
<font face="宋体" size=5>
    <p>&emsp;&emsp;上面介绍了CART算法的一些和C4.5不同之处，下面我们看看CART分类树建立算法的具体流程，之所以加上了建立，是因为CART树算法还有独立的剪枝算法这一块，这块我们在第5节讲。</p>
    <p>&emsp;&emsp;算法输入是训练集D，基尼系数的阈值，样本个数阈值。</p>
    <p>&emsp;&emsp;输出是决策树T。</p>
    <p>&emsp;&emsp;我们的算法从根节点开始，用训练集递归的建立CART树。</p>
    <p>&emsp;&emsp;1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。</p>
    <p>&emsp;&emsp;2) 计算样本集D的基尼系数，如果基尼系数大于阈值，则返回决策树子树，当前节点停止递归。</p>
    <p>&emsp;&emsp;3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第4.2节。缺失值的处理方法和第三节C4.5算法里描述的相同。</p>
    <p>&emsp;&emsp;4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.</p>
    <p>&emsp;&emsp;5) 对左右的子节点递归的调用1-4步，生成决策树。</p>
    <p>&emsp;&emsp;对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。</p>
</font>


### 4.4 CART回归树建立算法
<font face="宋体" size=5>
    <p>&emsp;&emsp;CART回归树和CART分类树的建立算法大部分是类似的，所以这里我们只讨论CART回归树和CART分类树的建立算法不同的地方。</p>
    <p>&emsp;&emsp;首先，我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。</p>
    <p>&emsp;&emsp;除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：</p>
    <p>&emsp;&emsp;1)连续值的处理方法不同。</p>
    <p>&emsp;&emsp;2)决策树建立后做预测的方式不同。</p>
    <p>&emsp;&emsp;对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i&space;\in&space;D_1(A,s)}(y_i&space;-&space;c_1)^2&space;&plus;&space;\underbrace{min}_{c_2}\sum\limits_{x_i&space;\in&space;D_2(A,s)}(y_i&space;-&space;c_2)^2\Bigg]" title="\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]" />
        </div>
    </p>
    <p>&emsp;&emsp;其中，<img src="http://latex.codecogs.com/gif.latex?\inline&space;c_1" title="c_1" />为D1数据集的样本输出均值，<img src="http://latex.codecogs.com/gif.latex?\inline&space;c_2" title="c_2" />为D2数据集的样本输出均值。</p>
    <p>&emsp;&emsp;对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
    <p>&emsp;&emsp;除了上面提到了以外，CART回归树和CART分类树的建立算法和预测没有什么区别。</p>
</font>


### 4.5 CART树算法的剪枝
#### 4.5.1 CART算法的剪枝原理
<font face="宋体" size=5>
    <p>&emsp;&emsp;CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样，这里我们一起来讲。</p>
    <p>&emsp;&emsp;由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的返回能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
    <p>&emsp;&emsp;也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
    <p>&emsp;&emsp;首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?C_{\alpha}(T_t)&space;=&space;C(T_t)&space;&plus;&space;\alpha&space;|T_t|" title="C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|" />
        </div>
    </p>
    <p>&emsp;&emsp;其中，α为正则化参数，这和线性回归的正则化一样。<img src="http://latex.codecogs.com/gif.latex?\inline&space;C(T_t)" title="C(T_t)" />为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。<img src="http://latex.codecogs.com/gif.latex?\inline&space;|T_t|" title="|T_t|" />是子树T的叶子节点的数量。</p>
    <p>&emsp;&emsp;当<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha&space;=&space;0" title="\alpha = 0" />时，即没有正则化，原始的生成的CART树即为最优子树。当<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha&space;=&space;\infty" title="\alpha = \infty" />时，，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_{\alpha}(T)" title="C_{\alpha}(T)" />最小的一颗子树。</p>
    <p>&emsp;&emsp;看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_t" title="T_t" />，如果没有剪枝，它的损失是</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?C_{\alpha}(T_t)&space;=&space;C(T_t)&space;&plus;&space;\alpha&space;|T_t|" title="C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|" />
        </div>
    </p>
    <p>&emsp;&emsp;如果将其剪掉，仅仅保留根节点，则损失是</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?C_{\alpha}(T)&space;=&space;C(T)&space;&plus;&space;\alpha" title="C_{\alpha}(T) = C(T) + \alpha" />
        </div>
    </p>
    <p>&emsp;&emsp;当α=0或者α很小时，<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_{\alpha}(T_t)&space;<&space;C_{\alpha}(T)" title="C_{\alpha}(T_t) < C_{\alpha}(T)" />，当α大到一定的程度时，<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_{\alpha}(T_t)&space;=&space;C_{\alpha}(T)" title="C_{\alpha}(T_t) = C_{\alpha}(T)" />。也就是说，如果满足下式：</p>
    <p>
        <div align="center">
            <img src="http://latex.codecogs.com/gif.latex?\alpha&space;=&space;\frac{C(T)-C(T_t)}{|T_t|-1}" title="\alpha = \frac{C(T)-C(T_t)}{|T_t|-1}" />
        </div>
    </p>
    <p>&emsp;&emsp;<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_t" title="T_t" />和<img src="http://latex.codecogs.com/gif.latex?\inline&space;T" title="T" />有相同的损失函数，但是<img src="http://latex.codecogs.com/gif.latex?\inline&space;T" title="T" />节点更少，因此可以对<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_t" title="T_t" />进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点<img src="http://latex.codecogs.com/gif.latex?\inline&space;T" title="T" />。</p>
    <p>&emsp;&emsp;最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
</font>

#### 4.5.2 CART剪枝算法过程
<font face="宋体" size=5>
    <p>&emsp;&emsp;输入是CART树建立算法得到的原始决策树T。</p>
    <p>&emsp;&emsp;输出是最优子树<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_\alpha" title="T_\alpha" />。</p>
    <p>&emsp;&emsp;算法过程如下：</p>
    <p>&emsp;&emsp;1）初始化<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha_{min}=&space;\infty" title="\alpha_{min}= \infty" />，最优子树集合<img src="http://latex.codecogs.com/gif.latex?\inline&space;\omega=\{T\}" title="\omega=\{T\}" />。</p>
    <p>&emsp;&emsp;2）从叶子节点开始自下而上计算各内部节点t的训练误差损失函数<img src="http://latex.codecogs.com/gif.latex?\inline&space;C_{\alpha}(T_t)" title="C_{\alpha}(T_t)" />，（回归树为均方差，分类树为基尼系数）, 叶子节点数<img src="http://latex.codecogs.com/gif.latex?\inline&space;|T_t|" title="|T_t|" />，以及正则化阈值<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha=&space;min\{\frac{C(T)-C(T_t)}{|T_t|-1},&space;\alpha_{min}\}" title="\alpha= min\{\frac{C(T)-C(T_t)}{|T_t|-1}, \alpha_{min}\}" />，更新<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha_{min}=&space;\alpha" title="\alpha_{min}= \alpha" />。</p>
    <p>&emsp;&emsp;3）得到所有节点的α值的集合M。</p>
    <p>&emsp;&emsp;4）从M中选择最大的值<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha_k" title="\alpha_k" />，自上而下的访问子树t的内部节点，如果<img src="http://latex.codecogs.com/gif.latex?\inline&space;\frac{C(T)-C(T_t)}{|T_t|-1}&space;\leq&space;\alpha_k" title="\frac{C(T)-C(T_t)}{|T_t|-1} \leq \alpha_k" />时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到<img src="http://latex.codecogs.com/gif.latex?\inline&space;\alpha_k" title="\alpha_k" />对于的最优子树<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_k" title="T_k" />。</p>
    <p>&emsp;&emsp;5）最优子树集合<img src="http://latex.codecogs.com/gif.latex?\inline&space;\omega=\omega&space;\cup&space;T_k" title="\omega=\omega \cup T_k" />，<img src="http://latex.codecogs.com/gif.latex?\inline&space;M=&space;M&space;-\{\alpha_k\}" title="M= M -\{\alpha_k\}" />。</p>
    <p>&emsp;&emsp;6）如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω。</p>
    <p>&emsp;&emsp;7）采用交叉验证在ω选择最优子树<img src="http://latex.codecogs.com/gif.latex?\inline&space;T_\alpha" title="T_\alpha" />。</p>
</font>

### 4.6 CART算法小结
<font face="宋体" size=5>
    <p>&emsp;&emsp;上面我们对CART算法做了一个详细的介绍，CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。当然CART树最大的好处是还可以做回归模型，这个C4.5没有。下表给出了ID3，C4.5和CART的一个比较总结。希望可以帮助大家理解。</p>
    <table>
        <tr>
            <td>算法</td>
            <td>支持模型</td>
            <td>树结构</td>
            <td>特征选择</td>
            <td>连续值处理</td>
            <td>缺失值处理</td>
            <td>剪枝</td>
        </tr>
        <tr>
            <td>ID3</td>
            <td>分类</td>
            <td>多叉树</td>
            <td>信息增益</td>
            <td>不支持</td>
            <td>不支持</td>
            <td>不支持</td>
        </tr>
        <tr>
            <td>C4.5</td>
            <td>分类</td>
            <td>多叉树</td>
            <td>信息增益比</td>
            <td>支持</td>
            <td>支持</td>
            <td>支持</td>
        </tr>
        <tr>
            <td>CART</td>
            <td>分类、回归</td>
            <td>二叉树</td>
            <td>基尼系数、均方差</td>
            <td>支持</td>
            <td>支持</td>
            <td>支持</td>
        </tr>
    </table>
    <p>&emsp;&emsp;看起来CART算法高大上，那么CART算法还有没有什么缺点呢？有！主要的缺点我认为如下：</p>
    <p>&emsp;&emsp;1）应该大家有注意到，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样绝息到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。</p>
    <p>&emsp;&emsp;2）如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。</p>
</font>


<br>

## 5 决策树算法小结
<font face=" 宋体" size=5>
    <p>&emsp;&emsp;终于到了最后的总结阶段了，这里我们不再纠结于ID3, C4.5和 CART，我们来看看决策树算法作为一个大类别的分类回归算法的优缺点。这部分总结于scikit-learn的英文文档。</p>
    <p>&emsp;&emsp;首先我们看看决策树算法的优点：</p>
    <p>&emsp;&emsp;1）简单直观，生成的决策树很直观。</p>
    <p>&emsp;&emsp;2）基本不需要预处理，不需要提前归一化，处理缺失值。</p>
    <p>&emsp;&emsp;3）使用决策树预测的代价是<img src="http://latex.codecogs.com/gif.latex?\inline&space;O(log_2m)" title="O(log_2m)" />，m为样本数。</p>
    <p>&emsp;&emsp;4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</p>
    <p>&emsp;&emsp;5）可以处理多维度输出的分类问题。</p>
    <p>&emsp;&emsp;6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释。</p>
    <p>&emsp;&emsp;7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。</p>
    <p>&emsp;&emsp;8）对于异常点的容错能力好，健壮性高。</p>
    <p>&emsp;&emsp;我们再看看决策树算法的缺点:</p>
    <p>&emsp;&emsp;1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</p>
    <p>&emsp;&emsp;2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</p>
    <p>&emsp;&emsp;3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</p>
    <p>&emsp;&emsp;4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</p>
    <p>&emsp;&emsp;5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</p>   
</font>


<br>
<br>

### Reference:
<font  face="宋体" size=5>
    <p>《统计学习方法》</p>
    <a href="http://www.cnblogs.com/pinard/p/6050306.html">决策树算法原理(上)</a>
    
</font>
