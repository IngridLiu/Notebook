# Deep Learning基础：Activiation Function
<br>
&emsp;&emsp;激活函数通常有如下一些性质：
&emsp;&emsp;**非线性**：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x)，就不满足这个性质了。如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
&emsp;&emsp;**可微性**：当优化方法是基于梯度的时候，这个性质是必须的。
&emsp;&emsp;**单调性**：当激活函数是单调的时候，单层网络能够保证是凸函数。
&emsp;&emsp;**f(x)≈x**：当激活函数满足这个性质的时候，如果参数擦的初始化是random很小的值，那么神经网络的训练将会很高效。如果不满足这个性质，那么就需要很用心的去设置初始值。
&emsp;&emsp;**输出值的范围**：当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.
<br>
<br>

## 